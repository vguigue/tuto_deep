{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TE2ItlsI956"
   },
   "source": [
    "# Séance 1 :  Deep Learning - Introduction à Pytorch \n",
    "\n",
    "Les notebooks sont très largement inspirés des cours de **N. Baskiotis et B. Piwowarski**. Ils peuvent être complétés efficacement par les tutoriels *officiels* présents sur le site de pytorch:\n",
    "https://pytorch.org/tutorials/\n",
    "\n",
    "Au niveau de la configuration, toutes les installations doivent fonctionner sur Linux et Mac. Pour windows, ça peut marcher avec Anaconda à jour... Mais il est difficile de récupérer les problèmes.\n",
    "\n",
    "* Aide à la configuration des machines: [lien](https://dac.lip6.fr/master/environnement-deep/)\n",
    "* Alternative 1 à Windows: installer Ubuntu sous Windows:  [Ubuntu WSL](https://ubuntu.com/wsl)\n",
    "* Alternative 2: travailler sur Google Colab (il faut un compte gmail + prendre le temps de comprendre comment accéder à des fichers) [Colab](https://colab.research.google.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3Y9YOOHHhJKY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La version de torch est :  2.1.2\n",
      "Le calcul GPU est disponible ?  False\n",
      "Le calcul GPU est disponible ?  True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"La version de torch est : \",torch.__version__)\n",
    "print(\"Le calcul GPU est disponible ? \", torch.cuda.is_available())\n",
    "\n",
    "# pour les possesseurs de mac M1 avec la dernière version de pytorch:\n",
    "print(\"Le calcul GPU est disponible ? \", torch.backends.mps.is_available())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzhX7D8KKIvt"
   },
   "source": [
    "# A. Autograd et graphe de calcul\n",
    "Un élément central de pytorch est le graphe de calcul : lors du calcul d'une variable, l'ensemble des opérations qui ont servies au calcul sont stockées sous la forme d'un graphe acyclique, dit de *calcul*. Les noeuds internes du graphe représentent les opérations, le noeud terminal le résultat et les racines les variables d'entrées. Ce graphe sert en particulier à calculer les dérivées partielles de la sortie par rapport aux variables d'entrées - en utilisant les règles de dérivations chainées des fonctions composées. \n",
    "Pour cela, toutes les fonctions disponibles dans pytorch comportent un mécanisme, appelé *autograd* (automatique differentiation), qui permet de calculer les dérivées partielles des opérations. \n",
    "\n",
    "## A.1. Différenciation automatique\n",
    "(De manière simplifiée, pour les détails cf [la documentation](https://pytorch.org/docs/stable/notes/extending.html))\n",
    "\n",
    "Toute opération sous pytorch hérite de la classe **Function** et doit définir :\n",
    "* une méthode **forward(\\*args)** : passe avant, calcule le résultat de la fonction appliquée aux arguments \n",
    "* une méthode **backward(\\*args)** : passe arrière, calcule les dérivées partielles par rapport aux entrées. Les arguments de  cette méthode correspondent aux valeurs des dérivées suivantes dans le graphe de calcul. En particulier, il y a autant d'arguments à **backward**  que de sorties pour la méthode **forward** (rétro-propagation : on doit connaître les dérivés qui viennent  en aval du calcul) et autant de sorties que d'arguments dans la méthode **forward** (chaque sortie correspond à  une dérivée partielle par rapport à chaque entrée du module). Le calcul se fait sur les valeurs du dernier appel de **forward**. \n",
    "\n",
    "Par exemple, pour la fonction d'addition  **add(x,y)**, **add.forward(x,y)** renverra **x+y** (l'appel de la fonction est équivalent à l'appel de **forward**) et **add.backward(1)** renverra le couple **(1,1)** (la dérivée par rapport à x, et celle par rapport à y) .\n",
    "\n",
    "En pratique, ce ne sont pas les méthodes de ces fonctions qui sont utilisées, mais des méthodes équivalentes sur les tenseurs. La méthode **backward** d'un tenseur permet de rétro-propager le calcul du gradient sur toutes les variables qui ont servies à son calcul.\n",
    "\n",
    "La valeur du gradient pour chaque dérivée partielle se trouve dans l'attribut **grad** de la variable concernée. \n",
    "\n",
    "Comme c'est un mécanisme lourd, l'autograd n'est pas activé par défaut pour une variable. Afin de l'activer, il faut mettre le flag **requires_grad** de cette variable à vrai. Dès lors, tout calcul qui utilise cette variable sera enregistré dans le graphe de calcul et le gradient sera disponible.\n",
    "\n",
    "\n",
    "Exemple : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "J_mYVeXMfsTV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphe de calcul ?  False\n",
      "Dérivée de z/a :  2.0  z/b : 1.0\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(1.)\n",
    "# Par défaut, requires_grad est à False\n",
    "print(\"Graphe de calcul ? \",a.requires_grad)\n",
    "# On peut demander à ce que le graphe de calcul soit retenu\n",
    "a.requires_grad = True \n",
    "# Ou lors de la création du tenseur directement\n",
    "b = torch.tensor(2.,requires_grad=True)\n",
    "z = 2*a + b\n",
    "# Calcul des dérivées partielles par rapport à z\n",
    "z.backward()\n",
    "print(\"Dérivée de z/a : \", a.grad.item(),\" z/b :\", b.grad.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Si on a oublié de demander le graphe de calcul :\n",
    "a, b = torch.tensor(1.),torch.tensor(2.)\n",
    "z = 2*a+b\n",
    "try: # on sait que ça va provoquer une erreur\n",
    "  z.backward()\n",
    "except Exception as e: # erreur => simple message\n",
    "  print(\"Erreur : \", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eApYmHIZa917"
   },
   "source": [
    "## A.2.  <span style=\"color:red\">     Exo : </span> Utilisation de backward     \n",
    "* Implémentez (en une ligne) la fonction de coût aux moindres carrés $MSE(\\hat{y},y)=\\frac{1}{2N} \\sum_{i=1}^N\\|\\hat{y_i}-y_i\\|^2$ où $\\hat{y},y$ sont deux matrices de taille $N\\times d$, et $y_i,\\hat{y_i}$ les $i$-èmes vecteurs lignes des matrices.\n",
    "* Engendrez **y,yhat** deux matrices aléatoires de taille $(1,5)$.\n",
    "* Calculez **MSE(y,yhat)**\n",
    "* Calculez à la main le gradient de **MSE** par rapport à **y**, **yhat**\n",
    "* Calculez grâce à pytorch le gradient de **MSE** par rapport à **y** et **yhat** et vérifier le résultat.\n",
    "* Appelez une deuxième fois **MSE** sur les mêmes vecteurs et la méthode **backward**. Qu'observez vous pour le gradient ? Comment l'expliquez vous ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Rd37M_3gkqu"
   },
   "outputs": [],
   "source": [
    "def MSE(yhat,y): # sur des vecteurs\n",
    "    # Compléter la fonction \n",
    "    ## <CORRECTION>\n",
    "    return ((yhat-y)**2).sum()/(2*yhat.size(0))\n",
    "    ## </CORRECTION>\n",
    "    pass\n",
    "\n",
    "y = torch.randn(1,5,requires_grad=True)\n",
    "yhat = torch.randn(1,5,requires_grad=True)\n",
    "mse = MSE(yhat,y)\n",
    "print(\"MSE :\" ,mse)\n",
    "\n",
    "# 1. retro-propager l'erreur\n",
    "# 2. afficher le gradient sur les deux vecteurs et comprendre ce qui se passe\n",
    "# 3. faire une itération supplémentaire et afficher de nouveau\n",
    "\n",
    "## <CORRECTION>\n",
    "mse.backward()\n",
    "print(f\"Dérivée MSE/y \\t\\t {y.grad},\\nDérivée MSE/yhat \\t {yhat.grad}, \\nmanuellement/yhat \\t {2*(yhat-y)}\")\n",
    "print('==== ITERATION 2 ====')\n",
    "mse = MSE(yhat,y)\n",
    "mse.backward()\n",
    "print(f\"Dérivée MSE/y \\t\\t {y.grad},\\nDérivée MSE/yhat  \\t {yhat.grad},\\nmanuellement/yhat \\t {2*(yhat-y)}\")\n",
    "## On n'a pas remis à zero le gradient, le gradient s'accumule dans .grad\n",
    "## </CORRECTION>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pour une compréhension du gradient en mode graphique\n",
    "\n",
    "Soit les points `A` et `B` de coordonnées respective $(1, 2)$ et $(4, 4)$. Si je construis une fonction qui minimise la distance entre $A$ et $B$ et que je calcule les gradients, j'obtiens des **directions** qu permettent de rapprocher les points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.tensor([1.,2], requires_grad=True)\n",
    "B = torch.tensor([4.,4], requires_grad=True)\n",
    "\n",
    "C = ((A-B)**2).sum()\n",
    "C.backward()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # en version graphique:\n",
    "    plt.figure()\n",
    "    plt.grid()\n",
    "    plt.scatter(A[0],A[1], s=100)\n",
    "    plt.scatter(B[0],B[1], s=100)\n",
    "    plt.quiver(A[0],A[1],-A.grad[0],-A.grad[1])\n",
    "    plt.quiver(B[0],B[1],-B.grad[0],-B.grad[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retour sur la régression polynomiale\n",
    "\n",
    "Soit un ensemble de points de $\\mathbb R^2$, $\\{A, B, C, D\\}$. Construire la fonction $y=a x^4 + b x^3, + c x^2 + d x + e$ passant par tous les points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.tensor([[1.,2]]) # tensor 2D pour pouvoir les concaténer\n",
    "B = torch.tensor([[2.,3]])\n",
    "C = torch.tensor([[3.,0]])\n",
    "D = torch.tensor([[4.,2]])\n",
    "\n",
    "# 1. Regrouper les points dans X\n",
    "# Faut-il activer le gradient sur ces points\n",
    "\n",
    "# <CORRECTION>\n",
    "X = torch.cat((A, B, C, D), dim=0)\n",
    "print(X)\n",
    "# </CORRECTION>\n",
    "\n",
    "# validation\n",
    "with torch.no_grad():\n",
    "    plt.figure()\n",
    "    plt.grid()\n",
    "    plt.scatter(X[:,0], X[:,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Construire la fonction de prediction et la fonction de cout\n",
    "# a) initialiser les coefficients a, b, c, d aléatoirement (randn)\n",
    "#       - faut-il activer le gradient sur ces coeffcients?\n",
    "# b) définir yhat = \n",
    "# c) définir cost = \n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# <CORRECTION>\n",
    "a = torch.randn(1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "c = torch.randn(1, requires_grad=True)\n",
    "d = torch.randn(1, requires_grad=True)\n",
    "yhat = a * X[:,0]**4 + b * X[:,0]**3 + c * X[:,0]**2 + d* X[:,0]+e\n",
    "\n",
    "cost = ((yhat - X[:,1])**2).mean()\n",
    "# </CORRECTION>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Affichage de ce régresseur (qui est aléatoire pour le moment)\n",
    "\n",
    "N = 100 # 100 points\n",
    "x = torch.linspace(0,5,N)\n",
    "y = a * x**4 + b * x**3 + c * x**2 + d *x +e\n",
    "\n",
    "with torch.no_grad():   \n",
    "    print(a, b, c, d, e)\n",
    "    plt.figure()\n",
    "    plt.grid()\n",
    "    plt.scatter(X[:,0], X[:,1])\n",
    "    plt.plot(x,y, 'r--')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimisation\n",
    "niter = 30000\n",
    "eps = 1e-3\n",
    "\n",
    "for i in range(niter):\n",
    "    # définition de l'esptimateur et du cout: cost\n",
    "    # <CORRECTION>\n",
    "    yhat = a * X[:,0]**3 + b * X[:,0]**2 + c * X[:,0] + d\n",
    "    cost = ((yhat - X[:,1])**2).mean()\n",
    "    # </CORRECTION>\n",
    "    cost.backward()\n",
    "    # mise à jour des paramètres\n",
    "    with torch.no_grad():\n",
    "        a -=  0.05* eps * a.grad\n",
    "        b -=  2  * eps * b.grad\n",
    "        c -=  10 * eps * c.grad\n",
    "        d -=  10 * eps * d.grad\n",
    "        \n",
    "    # penser à remettre les gradients à 0\n",
    "    a.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "    c.grad.zero_()\n",
    "    d.grad.zero_()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# affichage du résultat\n",
    "y = a * x**3 + b * x**2 + c * x + d \n",
    "\n",
    "with torch.no_grad():   \n",
    "    print(a, b, c, d)\n",
    "    plt.figure()\n",
    "    plt.grid()\n",
    "    plt.scatter(X[:,0], X[:,1])\n",
    "    plt.plot(x,y, 'r--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution analytique\n",
    "# comparons les performances et le temps de calcul\n",
    "# SGD = le 4x4 de l'optimisation... Pas forcément la meilleure solution !\n",
    "\n",
    "X0 = X[:,0].view(-1,1)\n",
    "X1 = X[:,1].view(-1,1)\n",
    "X2 = torch.cat(( X0**3, X0**2, X0, torch.ones(X0.size())), dim=1)\n",
    "print(X2)\n",
    "w = torch.inverse(X2.T@X2) @ X2.T @ X1\n",
    "\n",
    "a=w[0]\n",
    "b=w[1]\n",
    "c=w[2]\n",
    "d=w[3]\n",
    "# e=w[4]\n",
    "\n",
    "# affichage du résultat\n",
    "y = a * x**3 + b * x**2 + c * x + d \n",
    "\n",
    "with torch.no_grad():   \n",
    "    print(a, b, c, d, e)\n",
    "    plt.figure()\n",
    "    plt.grid()\n",
    "    plt.scatter(X[:,0], X[:,1])\n",
    "    plt.plot(x,y, 'r--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxgCbApIgU5X"
   },
   "source": [
    "\n",
    "## A.3. <span style=\"color:red\">  Exo : </span>  Régression linéaire en pytorch \n",
    "\n",
    "* Définissez la fonction **flineaire(x,w,b)** fonction linéaire qui calcule $f(x,w,b)=x.w^t+b$  avec $x\\in \\mathbb{R}^{{n\\times d}},~w\\in\\mathbb{R}^{1,d}, b\\in \\mathbb{R}$\n",
    "* Complétez le code ci-dessous pour réaliser une descente de gradient et apprendre les paramètres optimaux de la regression linéaire : $$w^∗,b^∗=\\text{argmin}_{w,b}\\frac{1}{N} \\sum_{i=1}^N \\|f(x^i,w,b)-y^i\\|^2$$\n",
    "\n",
    "Pour tester votre code, utilisez le jeu de données très classique *housing*, le prix des loyers à housing en fonction de caractéristiques socio-économiques des quartiers. Le code ci-dessous permet de les charger.\n",
    "\n",
    "<span style=\"color:red\"> ATTENTION ! </span>\n",
    "* pour la mise-à-jour des paramètres, <span style=\"color:red\">vous ne pouvez pas faire directement</span> \n",
    "$$w = w-\\epsilon*gradient$$ \n",
    "(pourquoi ?). Vous devez passer par w.data qui permet de ne pas enregistrer les opérations dans le graphe de calcul (ou utiliser la méthode ```.detach()``` d'une variable qui permet de créer une copie détachée du graphe de calcul). \n",
    "* Note: il est aussi possible de faire:\n",
    "    ```\n",
    "    with torch.no_grad():\n",
    "        w -= eps*gradient\n",
    "    ```\n",
    "    * Désactivation temporaire du graph de calcul, on manipule les tensors comme des variables classiques\n",
    "    * ATTENTION à faire des ```-=``` ou ```+=``` => Si vous construisez un nouveau tenseur, il ne se reconnectera pas au graphe de calcul!\n",
    "* l'algorithme doit converger avec la valeur de epsilon fixée; si ce n'est pas le cas, il y a une erreur (la plupart du temps au niveau du calcul du coût).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dArOgSWNTVvb"
   },
   "outputs": [],
   "source": [
    "def flineaire(x,w,b):\n",
    "    ## <CORRECTION>\n",
    "    return (x @ w.T)+b\n",
    "    ## </CORRECTION>\n",
    "    pass\n",
    "\n",
    "## Chargement des données housing (depuis sklearn) et transformation en tensor.\n",
    "# from sklearn.datasets import load_housing # => removed\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "housing = fetch_california_housing(data_home=\"./data/\") ## chargement des données\n",
    "\n",
    "# séparation des ensembles de données\n",
    "X_train, X_test, y_train, y_test = train_test_split( housing['data'], housing['target'],\n",
    "                                                                 test_size=0.33, random_state=42)\n",
    "\n",
    "housing_x = torch.tensor(X_train ,dtype=torch.float) # penser à typer les données pour éliminer les incertitudes\n",
    "housing_y = torch.tensor(y_train,dtype=torch.float)\n",
    "housing_xT = torch.tensor(X_test,dtype=torch.float) # penser à typer les données pour éliminer les incertitudes\n",
    "housing_yT = torch.tensor(y_test,dtype=torch.float)\n",
    "\n",
    "print(\"Nombre d'exemples : \",housing_x.size(0), \"Dimension : \",housing_x.size(1))\n",
    "print(\"Nom des attributs : \", \", \".join(housing['feature_names']))\n",
    "\n",
    "print(housing_x[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EPOCHS = 5000\n",
    "EPS = 1e-7 # que se passe-t-il lorsqu'on joue avec EPS?\n",
    "\n",
    "# initialisation aléatoire de w et b\n",
    "w = torch.randn(1,housing_x.size(1),requires_grad=True)\n",
    "b =  torch.randn(1,1,requires_grad=True)\n",
    "loss_h = [] # sauvegarde des valeurs de loss (pas si trivial!)\n",
    "\n",
    "# boucle de descente de gradient\n",
    "for i in range(EPOCHS):\n",
    "    pass\n",
    "    ## SOLUTION 1: Penser à aller chercher w.data (et sa contrepartie dans le gradient)\n",
    "    # 1. Construire la loss (+stocker la valeur dans loss_h)\n",
    "    # 2. Retro-propager\n",
    "    # 3. MAJ des paramètres\n",
    "    # 4. Penser à remettre le gradient à 0 (cf exo précédent)\n",
    "    ## <CORRECTION>\n",
    "    loss =  MSE(flineaire(housing_x,w,b).view(-1,1),housing_y.view(-1,1))\n",
    "    loss_h.append(loss.detach().numpy()) # pas si facile de revenir en numpy lorsque la structure calcul un gradient\n",
    "    if i % 100==0:  print(f\"iteration : {i}, loss : {loss}\")\n",
    "    #calcul du gradient\n",
    "    loss.backward()\n",
    "    # Maj des paramètres (à la main)\n",
    "    w.data = w.data-EPS*w.grad.data # les variables sont divisées en data|grad.data\n",
    "    b.data = b.data-EPS*b.grad.data\n",
    "    # annulation du gradient (pour éviter l'accumulation d'une itération à l'autre)\n",
    "    w.grad.data.zero_() # fonction en _ pour le inplace\n",
    "    b.grad.data.zero_()\n",
    "    ## </CORRECTION>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# une seconde version du même code avec l'environnement torch.no_grad()\n",
    "# attention, dans ce cas, le += est obligatoire\n",
    "# code identique (juste changer les 2 lignes de MAJ)\n",
    "\n",
    "EPOCHS = 5000\n",
    "EPS = 1e-7\n",
    "#initialisation aléatoire de w et b\n",
    "w = torch.randn(1,housing_x.size(1),requires_grad=True)\n",
    "b =  torch.randn(1,1,requires_grad=True)\n",
    "loss_h = [] # sauvegarde des valeurs de loss (pas si trivial!)\n",
    "for i in range(EPOCHS):\n",
    "    pass\n",
    "    ## SOLUTION 2: avec torch.no_grad() [toutes les lignes sont identiques, sauf les 2 lignes de MAJ des paramètres]\n",
    "    ## <CORRECTION>\n",
    "    loss =  MSE(flineaire(housing_x,w,b).view(-1,1),housing_y.view(-1,1))\n",
    "    loss_h.append(loss.detach().numpy()) # pas si facile de revenir en numpy lorsque la structure calcul un gradient\n",
    "    if i % 100==0:  print(f\"iteration : {i}, loss : {loss}\")\n",
    "    #calcul du gradient\n",
    "    loss.backward()\n",
    "    # Maj des paramètres (à la main)\n",
    "    with torch.no_grad():\n",
    "        w += -EPS*w.grad\n",
    "        b += -EPS*b.grad\n",
    "    # annulation du gradient (pour éviter l'accumulation d'une itération à l'autre)\n",
    "    w.grad.data.zero_() # fonction en _ pour le inplace\n",
    "    b.grad.data.zero_()\n",
    "    ## </CORRECTION>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# affichage de l'optimisation\n",
    "plt.figure()\n",
    "plt.plot(loss_h)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"mse loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70il-LA_XGSO"
   },
   "source": [
    "\n",
    "## Optimiseur \n",
    "La descente de gradient représente en fait un code standard puisque les dérivées sont calculées automatiquement et que les variables sont idéntifiées.\n",
    "Pytorch inclut une classe très utile pour la descente de gradient, [torch.optim](https://pytorch.org/docs/stable/optim.html), qui permet :\n",
    "* d'économiser quelques lignes de codes\n",
    "* d'automatiser la mise-à-jour des paramètres \n",
    "* d'abstraire le type de descente de gradient utilisé (sgd,adam, rmsprop, ...)\n",
    "\n",
    "Une liste de paramètres à optimiser est passée à l'optimiseur lors de l'initialisation. La méthode **zero_grad()** permet de remettre le gradient à zéro et la méthode **step()** permet de faire une mise-à-jour des paramètres.\n",
    "\n",
    "Un exemple de code  utilisant l'optimiseur est donné ci-dessous. Testez et comparez les résultats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "58sP5ryLeP3Y"
   },
   "outputs": [],
   "source": [
    "Xdim = housing_x.size(1)\n",
    "\n",
    "w = torch.randn(1,Xdim,dtype=torch.float,requires_grad=True)\n",
    "b = torch.randn(1,dtype=torch.float,requires_grad=True)\n",
    "\n",
    "## on optimise selon w et b.  lr est le pas du gradient\n",
    "optim = torch.optim.SGD(params=[w,b],lr=EPS) \n",
    "for i in range(EPOCHS):\n",
    "  loss = MSE(flineaire(housing_x,w,b).view(-1,1),housing_y.view(-1,1))\n",
    "  optim.zero_grad()\n",
    "  loss.backward()\n",
    "  optim.step()  \n",
    "  if i % 100==0:  print(f\"iteration : {i}, loss : {loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mise en perspective\n",
    "\n",
    "Les résultats obtenus sont-ils bons??\n",
    "\n",
    "- comparaison avec un modèle naif = prédiction de la moyenne\n",
    "- comparaison avec un modèle linéaire\n",
    "- comparaison avec une forêt aléatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# modele moyen\n",
    "err = mean_squared_error(y_test, y_test.mean()*np.ones(y_test.shape))\n",
    "print(\"err / moyenne : \", err)\n",
    "\n",
    "# modèle linéaire\n",
    "mod = LinearRegression()\n",
    "mod.fit(X_train,y_train)\n",
    "yhat = mod.predict(X_test)\n",
    "err = mean_squared_error(y_test, yhat)\n",
    "print(\"err / lin : \", err)\n",
    "\n",
    "# gradient boosting\n",
    "mod = GradientBoostingRegressor()\n",
    "mod.fit(X_train,y_train)\n",
    "yhat = mod.predict(X_test)\n",
    "err = mean_squared_error(y_test, yhat)\n",
    "print(\"err / grad boost : \", err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelques propositions rapides pour être moins ridicule !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xdim = housing_x.size(1)\n",
    "\n",
    "w = torch.randn(1,Xdim,dtype=torch.float,requires_grad=True)\n",
    "b = torch.randn(1,dtype=torch.float,requires_grad=True)\n",
    "with torch.no_grad(): # adoucir l'initialisation => optimisation plus facile\n",
    "  w*=0.1\n",
    "  b*=0.1\n",
    "\n",
    "## on optimise selon w et b.  lr est le pas du gradient\n",
    "# optim = torch.optim.SGD(params=[w,b],lr=EPS) \n",
    "optim = torch.optim.Adam(params=[w,b],lr=1e-3) # un peu plus efficace\n",
    "for i in range(EPOCHS):\n",
    "  loss = MSE(flineaire(housing_x,w,b).view(-1,1),housing_y.view(-1,1))\n",
    "  optim.zero_grad()\n",
    "  loss.backward()\n",
    "  optim.step()  \n",
    "  if i % 100==0:  print(f\"iteration : {i}, loss : {loss}\")\n",
    "\n",
    "print(\"Résultat en test : \", MSE(flineaire(housing_xT,w,b).view(-1,1),housing_yT.view(-1,1)).item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction du sujet à partir de la correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### <CORRECTION> ###\n",
    "import re\n",
    "# transformation de cet énoncé en version étudiante\n",
    "\n",
    "fname = \"1_2-pytorch-grad-corr.ipynb\" # ce fichier\n",
    "fout  = fname.replace(\"-corr\",\"\")\n",
    "\n",
    "# print(\"Fichier de sortie: \", fout )\n",
    "\n",
    "f = open(fname, \"r\")\n",
    "txt = f.read()\n",
    " \n",
    "f.close() \n",
    "\n",
    "\n",
    "f2 = open(fout, \"w\")\n",
    "f2.write(re.sub(\"<CORRECTION>.*?(</CORRECTION>)\",\" TODO \",\\\n",
    "    txt, flags=re.DOTALL))\n",
    "f2.close()\n",
    "\n",
    "### </CORRECTION> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DeepLearning fc TP1 2020-2021-correction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.15 ('torch-nightly')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "0539fa28f95d3c31ab10ef7831d62bf0e47751f442c28c9f64a267247464e7da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
