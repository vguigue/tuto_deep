{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representation learning & recommender systems\n",
    "\n",
    "In this practical session, we investigate two classical matrix-factorization models and their neural network implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install torch torchvision pytorch-lightning --upgrade\n",
    "#! pip install matplotlib --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data used : [smallest movie-lens dataset](https://grouplens.org/datasets/movielens/)\n",
    "\n",
    "Let's start with a very common dataset describing users, movies & interactions (ratings):\n",
    "\n",
    "![image reco](media/Facto-mat.png)\n",
    "\n",
    "# 1)  Load & Prepare Data\n",
    "\n",
    "To be able to embed the data easily, we need to remap  the user/items between [0->N_User] and [0->N_Items]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "## Load\n",
    "#ratings = pd.read_csv(\"data/ratings.csv\")\n",
    "ratings = pd.read_csv(\"data/ml-100k/u.data\", sep=\"\\t\",dtype=int, names=[\"userId\",\"movieId\", \"rating\", \"timestamp\"])\n",
    "# We use pandas to load the data... And that's it => No pandas requirements for this practical session !\n",
    "\n",
    "ratings.astype({'rating': 'float'},copy=False)\n",
    "ratings.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Prepare Data\n",
    "user_map = {user:num for num,user in enumerate(ratings[\"userId\"].unique())}\n",
    "item_map = {item:num for num,item in enumerate(ratings[\"movieId\"].unique())}\n",
    "\n",
    "## Number of users & items\n",
    "num_users = len(user_map)\n",
    "num_items = len(item_map)\n",
    "\n",
    "ratings[\"userId\"] = ratings[\"userId\"].map(user_map)\n",
    "ratings[\"movieId\"] = ratings[\"movieId\"].map(item_map)\n",
    "\n",
    "ratings.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating Test/Train as before\n",
    "\n",
    "train_indexes,val_indexes,test_indexes = [],[],[]\n",
    "\n",
    "for index in range(len(ratings)):\n",
    "    if index%5 == 0: # 20% of the data\n",
    "        test_indexes.append(index)\n",
    "    else:\n",
    "        train_indexes.append(index)\n",
    "\n",
    "        \n",
    "shuffle(train_indexes)\n",
    "num_val = int(len(train_indexes)/100*20)\n",
    "val_indexes = train_indexes[:num_val]\n",
    "train_indexes = train_indexes[num_val:]\n",
    "\n",
    "train_ratings = ratings.iloc[train_indexes].copy() # separate data\n",
    "val_ratings = ratings.iloc[val_indexes].copy()\n",
    "test_ratings = ratings.iloc[test_indexes].copy()\n",
    "\n",
    "\n",
    "print(f\" #train:{len(train_ratings)}, #val:{len(val_ratings)} ,#test:{len(test_ratings)}\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USAGE\n",
    "# In what follows, we will browse the tuple this way:\n",
    "cpt = 0\n",
    "for index, uid, mid, r, ts in train_ratings.itertuples():\n",
    "    print(index,uid, mid,r) # remember that indexes were shuffled\n",
    "    cpt+=1\n",
    "    if cpt > 5:\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduce the baseline model with pytorch's vanilla autograd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your goal now is to reproduce the following (strong) baseline model from surprise\n",
    "\n",
    " $$\\hat{r}_{ui} = b_{ui} = \\mu + b_u + b_i, \\qquad (\\mu,b_u,b_i) \\in \\mathbb R^3$$\n",
    "\n",
    "[no matrix factorization here, <font color=\"red\">only 3 scalars</font> involved for a prediction $(u,i)$] <BR>\n",
    "[Even $\\mu$ could be computed from the train set, we are going to learn this parameter in the optimization process]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, let's define the parameters\n",
    "\n",
    "You have many parameters, they are all 1-dimensional:\n",
    "- **mu:** the global mean (1,)\n",
    "- **bu:** the user means (n_users,)\n",
    "- **bi:** the item means (n_items,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = torch.tensor([3.5],requires_grad=True) # activate gradient to be able to learn something\n",
    "bu = [torch.tensor([0.1],requires_grad=True) for _ in range(num_users)]\n",
    "bi = [torch.tensor([0.1],requires_grad=True) for _ in range(num_items)]\n",
    "\n",
    "# # using directly the embedding module it would give:\n",
    "# KEEP custom tensor first => Easier index management\n",
    "# mu = torch.nn.Embedding(1, 1) # only one scalar to learn for the whole set\n",
    "# bu = torch.nn.Embedding(num_users, 1) # one scalar per user\n",
    "# bi = torch.nn.Embedding(num_items, 1) # one scalar per user\n",
    "\n",
    "# # init\n",
    "# torch.nn.init.normal_(mu.weight,3.5,0.001) # almost cst\n",
    "# torch.nn.init.normal_(bu.weight,0.1,0.001) \n",
    "# torch.nn.init.normal_(bi.weight,0.1,0.001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check the dimensions of the created structures\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Then, we define two functions: \n",
    "\n",
    "- `predict(u,i)` : Will return the prediction given the (user,item) pair\n",
    "- `error(pred,real)` : Will return the MSE error of prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (TODO) Predict Function\n",
    "This function should implement this: $\\hat{r}_{ui} = b_{ui} = \\mu + b_u + b_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(u,i):\n",
    "    # build a (simlple) prediction from the above mentioned parameters\n",
    "    if u < num_users: # if user exist:\n",
    "        user_mean = bu[u] # brakets with custom tensors\n",
    "        # user_mean = bu(u) # parentheses with embedding... But also squeeze/unsqueeze required\n",
    "    else:\n",
    "        user_mean = 0 \n",
    "        \n",
    "    if i < num_items: # if item exist:\n",
    "        item_mean = bi[i]\n",
    "    else:\n",
    "        item_mean = 0\n",
    "    \n",
    "    return mu + user_mean + item_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation on user 0 and item 0 \n",
    "print(predict(0,0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (TODO) error function\n",
    "We want to use the MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(pred,real):\n",
    "    # define simple MSE (1 line, few chars)\n",
    "    ## <CORRECTION>\n",
    "    return (pred-real)**2\n",
    "    ## </CORRECTION>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The evaluation loop, without any optimization for now\n",
    "\n",
    "Bad results expected... Just to check if we can use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_e = 0\n",
    "for index, uid, mid, r, ts in train_ratings.itertuples(): # elegant way to browse tuples (from pandas structure)\n",
    "    result = predict(uid,mid)\n",
    "    train_e += error(result,r).item()\n",
    "\n",
    "# define the same command for validation, test [copy/past/minor changes on var names]\n",
    "# display the errors    \n",
    "# The 3 errors are likely to be close with no learning step\n",
    "## <CORRECTION>\n",
    "val_e = 0\n",
    "for index, uid, mid, r, ts in val_ratings.itertuples():\n",
    "    result = predict(uid,mid)\n",
    "    val_e += error(result,r).item()\n",
    "\n",
    "test_e = 0\n",
    "for index, uid, mid, r, ts in test_ratings.itertuples():\n",
    "    result = predict(uid,mid)\n",
    "    test_e += error(result,r).item()\n",
    "\n",
    "print(\"final train error : \", train_e/len(train_ratings))\n",
    "print(\"final val error : \", val_e/len(val_ratings))\n",
    "print(\"final test error : \", test_e/len(test_ratings))\n",
    "## </CORRECTION>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's optimize the parameters (with SGD)  by slightly modifying the previous loop\n",
    "\n",
    "### (TODO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters' values\n",
    "lr = 0.01\n",
    "batch_size = 32\n",
    "n_epochs = 5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    # loop on the training samples (cf above)\n",
    "    #   prediction\n",
    "    #   error\n",
    "    #   [OPT] error storage to check convergence\n",
    "    #   backward (accumulation)\n",
    "    #   update\n",
    "    #   zero_grad\n",
    "\n",
    "    # <CORRECTION>\n",
    "    train_e = 0\n",
    "    for num,(index, uid, mid, r, ts) in enumerate(train_ratings.sample(frac=1).itertuples()):\n",
    "        result = predict(uid,mid)\n",
    "        se = error(result,r)\n",
    "        train_e += se.item()\n",
    "        se.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mu -= lr*mu.grad\n",
    "            bu[uid] -= lr*bu[uid].grad\n",
    "            bi[mid] -= lr*bi[mid].grad\n",
    "\n",
    "            # Manually zero the gradients after updating weights\n",
    "            mu.grad.zero_()\n",
    "            bu[uid].grad.zero_()\n",
    "            bi[mid].grad.zero_()\n",
    "\n",
    "\n",
    "    print(f\"epoch {epoch} train error : \", train_e/len(train_ratings))\n",
    "    # </CORRECTION>\n",
    "\n",
    "    # Evalaution on the validation set + test set\n",
    "    val_e = 0\n",
    "    for index, uid, mid, r, ts in val_ratings.itertuples():\n",
    "        result = predict(uid,mid)\n",
    "        val_e += error(result,r).item()\n",
    "\n",
    "    print(f\"epoch {epoch} val error : \", val_e/len(val_ratings))\n",
    "\n",
    "    test_e = 0\n",
    "    for index, uid, mid, r, ts in test_ratings.itertuples():\n",
    "        result = predict(uid,mid)\n",
    "        test_e += error(result,r).item()\n",
    "\n",
    "    print(f\"epoch {epoch} test error : \", test_e/len(test_ratings))\n",
    "    print(\"-----\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding module\n",
    "\n",
    "To build a matrix of vectorial representations of dimension $Z$, for instance describing the users, we are going to use a new module called `embedding`:\n",
    "$$ U = \\begin{pmatrix}\\mathbf u_1, \\ldots, \\mathbf u_n\\end{pmatrix}, \\mathbf u \\in \\mathbb R^Z $$ \n",
    "\n",
    "Call for a index, get a $Z$ dimensional representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_size = 10\n",
    "nb_users = 100 # arbitrary\n",
    "nb_items = 50\n",
    "users = torch.nn.Embedding(nb_users, latent_size) # random init\n",
    "items = torch.nn.Embedding(nb_items, latent_size) # random init\n",
    "\n",
    "# get representation of user 5:\n",
    "print(\"User 5:\", users(torch.tensor(5))) # WARNING: call for a tensor (not an int)\n",
    "\n",
    "# get representation of user 5 & 7:\n",
    "print(\"User 5 & 7:\", users(torch.tensor([5,7])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding with smaller values:\n",
    "\n",
    "torch.nn.init.normal_(users.weight,0,0.01) # apply on the weights\n",
    "\n",
    "# get representation of user 5:\n",
    "print(\"User 5:\", users(torch.tensor(5))) # WARNING: call for a tensor (not an int)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main difficulty = dealing with batch !\n",
    "\n",
    "Based on a very simple matrix factorization formulation:\n",
    "$$ \\hat r = I_i^T U_u $$\n",
    "Are you able to compute $\\hat r$ for a batch of index `ind`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = torch.tensor([1,2,3])\n",
    "print(ind, ind.size())\n",
    "print(users(ind).size(), users(ind))\n",
    "\n",
    "u = users(ind)\n",
    "i = items(ind)\n",
    "\n",
    "# compute i.T u for all indices\n",
    "# idea to save useless computations:\n",
    "# 1. pairwise multiplication\n",
    "# 2. find the good sum to get correct dimensions (and probably correct results)\n",
    "# <CORRECTION>\n",
    "r = torch.sum(u*i,1)\n",
    "print(r.size(), r)\n",
    "# </CORRECTION>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific syntax to deal with an undefined number of parameters\n",
    "\n",
    "widely used in python... And in particular in the next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. a fuction return many things (in a tuple)\n",
    "\n",
    "def fonction():\n",
    "    return 1, 2, 3, 4\n",
    "# default behavior => get a tuple:\n",
    "a, b, c, d = fonction()\n",
    "print(a, b, c, d)\n",
    "\n",
    "#2. you can store them in a list (but not the first)\n",
    "_,*res = fonction()\n",
    "\n",
    "# 3. create a new function that takes arbitrary number of parameters:\n",
    "def fonction2(*params):\n",
    "    for p in params:\n",
    "        print(p)\n",
    "\n",
    "fonction2(res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  Classic matrix factorisation (called SVD in RecSys) (with mean)\n",
    "\n",
    "To see how it works, we propose to implement a simple SVD:\n",
    "### $$ \\min\\limits_{U,I}\\sum\\limits_{(u,i)} \\underbrace{(r_{ui} -  (I_i^TU_u + \\mu))^2}_\\text{minimization} + \\underbrace{\\lambda(||U_u||^2+||I_u||^2 + \\mu) }_\\text{regularization} $$\n",
    "\n",
    "where prediction is done in the following way:\n",
    "### $$r_{ui} = \\mu + U_u.I_i $$\n",
    "\n",
    "where $\\mu$ is the global mean,  $U_u$ a user embedding and $I_i$ an item embedding\n",
    "\n",
    "### STEPS:\n",
    " To implement such model in pytorch, we need to do multiple things:\n",
    " \n",
    " - (1) model definition\n",
    " - (2) loss function\n",
    " - (3) evaluation\n",
    " - (4) training/eval loop\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### (1) Model definition\n",
    "\n",
    "A model class typically extends `nn.Module`, the Neural network module. It is a convenient way of encapsulating parameters, with helpers for moving them to GPU, exporting, loading, etc.\n",
    "\n",
    "One should define two functions: `__init__` and `forward`.\n",
    "\n",
    "- `__init__` is used to initialize the model parameters\n",
    "- `forward` is the net transformation from input to output. In fact, when doing `moduleClass(input)` you call this method.\n",
    "\n",
    "##### (a) Initialization\n",
    "\n",
    "Our model has different weigths:\n",
    "\n",
    "- the user profiles (also called user embeddings) $U$\n",
    "- the item profiles (also called user embeddings) $I$\n",
    "- the mean bias $\\mu$\n",
    "\n",
    "\n",
    "##### (b) input to output operation\n",
    "Technically, the prediction as defined earlier can be seen as just a dot product between two embeddings $U_u$ and $I_i$ plus the mean rating:\n",
    "\n",
    "- `torch.sum(embed_u*embed_i,1) + self.mean` is equivalent to $r_{ui} = \\mu + U_u.I_i $ \n",
    "- the `.squeeze(1)` operation is a shape operation to remove the dimension 1 (indexing starts at 0) akin to reshaping the matrix from `(batch_size,1,latent_size)` to `(batch_size,latent_size)`\n",
    "- for reference, the inverse operation is `.unsqueeze()`\n",
    "- we return weights to regularize them\n",
    "\n",
    "\n",
    "### (TODO) Just to make sure you were following: complete the following `__init__`and  `forward` methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# The model define as a class, inheriting from nn.Module\n",
    "class ClassicMF(torch.nn.Module):\n",
    "    \n",
    "    #(a) Init\n",
    "    def __init__(self,nb_users,nb_items,latent_size):\n",
    "        super(ClassicMF, self).__init__()\n",
    "        # define the embeddings\n",
    "        #   note: the general bias is given with specific syntax\n",
    "        #   note: to define an attribute: self.users = ...\n",
    "        # initialize with std = 0.01\n",
    "\n",
    "        #The mean bias\n",
    "        self.mean = torch.nn.Parameter(torch.FloatTensor(1,).fill_(3)) # another way to activate grad\n",
    "        # <CORRECTION>\n",
    "        #Embedding layers\n",
    "        self.users = torch.nn.Embedding(nb_users, latent_size)        \n",
    "        self.items = torch.nn.Embedding(nb_items, latent_size)\n",
    "        \n",
    "        #initialize weights with very small values\n",
    "        torch.nn.init.normal_(self.users.weight,0,0.01)\n",
    "        torch.nn.init.normal_(self.items.weight,0,0.01)\n",
    "        # </CORRECTION>\n",
    "    \n",
    "    # (b) How we compute the prediction (from input to output)\n",
    "    def forward(self, user, item): ## method called when doing ClassicMF(user,item)\n",
    "        # pay attention to the arguments: we have to give indexes\n",
    "        # from the indexes, compute the output\n",
    "        # WARNING : return the embeddings on top of the output to compute the regularization term \n",
    "        #       => 4 outputs expected (the line is given)\n",
    "\n",
    "       \n",
    "        # <CORRECTION>\n",
    "         #embed_u,embed_i = self.users(user).squeeze(1),self.items(item).squeeze(1) # old => unecessary\n",
    "        embed_u,embed_i = self.users(user),self.items(item)\n",
    "        # print(user, user.size())\n",
    "        # print(self.users(user).size(), self.users(user))\n",
    "        \n",
    "        out =   torch.sum(embed_u*embed_i,-1) + self.mean\n",
    "        # </CORRECTION>\n",
    "        return out, embed_u, embed_i, self.mean  # We return prediction + weights to regularize them\n",
    "       \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2-4) full train loop\n",
    "\n",
    "The train loop is organized around the [Dataloader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) class which Combines a dataset and a sampler, and provides single- or multi-process iterators over the dataset.\n",
    "\n",
    "We just redefine a collate function\n",
    "\n",
    "> collate_fn (callable, optional) – merges a list of samples to form a mini-batch.\n",
    "\n",
    "\n",
    "**NOTE:** The dataset argument can be a list instead of a \"Dataset\" instance (works by duck typing)\n",
    "    \n",
    "\n",
    "##### The train loop sequence is the following:\n",
    "    \n",
    "[Dataset ==Dataloader==> Batch (not prepared) ==collate_fn==> Batch (prepared) ==Model.forward==> Prediction =loss_fn=> loss <-> truth \n",
    "\n",
    "1] PREDICT\n",
    "- (a) The dataloader samples training exemples from the dataset (which is a list)\n",
    "- (b) The collate_fn prepares the minibatch of training exemples\n",
    "- (c) The prediction is made by feeding the minibatch in the model\n",
    "- (d) The loss is computed on the prediction via a loss function\n",
    "\n",
    "2] OPTIMIZE\n",
    "- (e) Gradients are computed by automatic backard propagation\n",
    "- (f) Parameters are updated using computed gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Let's create the datasets following  (Object w/ __getitem__(index) and __len()__, i.e lists ;)\n",
    "prep_train = [(tp.userId,tp.movieId,tp.rating) for tp in train_ratings.itertuples()]\n",
    "prep_val   = [(tp.userId,tp.movieId,tp.rating) for tp in val_ratings.itertuples()]\n",
    "prep_test  = [(tp.userId,tp.movieId,tp.rating) for tp in test_ratings.itertuples()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c = zip(*prep_train[:10])\n",
    "print(a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# HyperParameters\n",
    "n_epochs = 3\n",
    "batch_size = 16\n",
    "num_feat = 25\n",
    "lr = 0.01\n",
    "reg = 0.001\n",
    "\n",
    "\n",
    "#(a) Collate function => Creates tensor batches to feed model during training\n",
    "# It can be removed if data is already tensors (torch or numpy ;)\n",
    "def tuple_batch(l):\n",
    "    '''\n",
    "    input l: list of (user,item,rating tuples)\n",
    "    output: formatted batches (in torch tensors)\n",
    "\n",
    "    takes n-tuples and create batch\n",
    "    text -> seq word #id\n",
    "    '''\n",
    "    users, items, ratings = zip(*l) \n",
    "    users_t = torch.LongTensor(users)\n",
    "    items_t = torch.LongTensor(items)\n",
    "    ratings_t = torch.FloatTensor(ratings)\n",
    "    \n",
    "    return users_t, items_t, ratings_t\n",
    "    \n",
    "\n",
    "\n",
    "#(b) Loss function => Combines MSE and L2\n",
    "def loss_func(pred,ratings_t,reg,*params): # specific syntax \n",
    "    '''\n",
    "    mse loss combined with l2 regularization.\n",
    "    params assumed 2-dimension\n",
    "    '''\n",
    "    mse = F.mse_loss(pred,ratings_t,reduction='sum')\n",
    "    l2 = 0\n",
    "    for p in params: # ranging on all parameters\n",
    "        l2 += torch.mean(p.norm(2,-1))\n",
    "        \n",
    "    return (mse/pred.size(0)) + reg*l2 , mse\n",
    "    \n",
    "#\n",
    "# Training script starts here\n",
    "#    \n",
    "\n",
    "# (a) dataloader will sample data from datasets using collate_fn tuple_batch\n",
    "dataloader_train = DataLoader(prep_train, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=tuple_batch)\n",
    "dataloader_val = DataLoader(prep_val, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=tuple_batch)\n",
    "dataloader_test = DataLoader(prep_test, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=tuple_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model & optimizer\n",
    "\n",
    "model = ClassicMF(num_users,num_items,num_feat)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## INTERMEDIATE BOX for in depth understanding\n",
    "\n",
    "# inference & parameter retrieving (if your forward is defined as expected)\n",
    "users_t,items_t,ratings_t = next(iter(dataloader_train)) # retrieve first batch\n",
    "# check dim\n",
    "print(users_t.size()) # batch\n",
    "print(users_t)\n",
    "\n",
    "# output of the forward step:\n",
    "pred, embed_u, embed_i, mu = model(users_t,items_t)\n",
    "print(pred.size(), embed_u.size()) # batch\n",
    "print(pred) # Current predictions for the batch\n",
    "\n",
    "# alternative advanced syntax\n",
    "pred, *params = model(users_t,items_t) # param is a list !!\n",
    "print(len(params)) \n",
    "print(params[0].size()) # params[0] corresponds to embed_u\n",
    "\n",
    "# idea: retrieving the list of parameter... And then transmit the list to loss_func without unpacking\n",
    "print(loss_func(pred,ratings_t,reg,*params))    # yhat, y, lambda_reg, all_params\n",
    "                                                # return mse + regul, mse (sum not the mean)\n",
    "\n",
    "# we can apply backward on what we want...\n",
    "                                                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Train loop (epoch)\n",
    "#   loop over the dataloader\n",
    "#       forward (+get the parameters)\n",
    "#       loss\n",
    "#       backward\n",
    "#       optim\n",
    "#   compute mse on validation & test\n",
    "#   display losses for epoch e\n",
    "#\n",
    "\n",
    "##<CORRECTION>\n",
    "for e in range(n_epochs):\n",
    "    mean_loss = [0,0,0] #train/val/test\n",
    "\n",
    "    ## Training loss (the one we train with)\n",
    "    \n",
    "    for users_t,items_t,ratings_t in dataloader_train:\n",
    "        model.train() # set the model on train mode\n",
    "        model.zero_grad() # reset gradients\n",
    "        \n",
    "        #(c) predictions are made by the model\n",
    "        pred,*params = model(users_t,items_t)\n",
    "        \n",
    "        #(d) loss computed on predictions, we added regularization\n",
    "        loss,mse_loss = loss_func(pred,ratings_t,reg,*params)\n",
    "        \n",
    "        loss.backward() #(e) backpropagating to get gradients\n",
    "        \n",
    "        mean_loss[0] += mse_loss\n",
    "        optimizer.step() #(f) updating parameters\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        ## Validation loss (no training)\n",
    "        for users_t,items_t,ratings_t in dataloader_val:\n",
    "\n",
    "            model.eval() # Inference mode\n",
    "            pred,*params = model(users_t,items_t)\n",
    "            _,mse_loss = loss_func(pred,ratings_t,reg,*params)\n",
    "\n",
    "            mean_loss[1] += mse_loss    \n",
    "\n",
    "        ## Test loss (no training)\n",
    "\n",
    "        for users_t,items_t,ratings_t in dataloader_test:\n",
    "            model.eval()\n",
    "            pred,*params = model(users_t,items_t)\n",
    "            _,mse_loss = loss_func(pred,ratings_t,reg,*params)\n",
    "\n",
    "            mean_loss[2] += mse_loss    \n",
    "\n",
    "    print(\"-\"*25)\n",
    "    print(\"epoch\",e, \"mse (train/val/test)\", round((mean_loss[0]/len(prep_train)).item(),3),\"/\",  round((mean_loss[1]/len(prep_val)).item(),3),\"/\",  round((mean_loss[2]/len(prep_test)).item(),3))\n",
    "    ##</CORRECTION>\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Your turn from scratch) Koren 2009 model:\n",
    "\n",
    "Here, this model simply adds a bias for each user and for each item\n",
    "\n",
    "### $$ \\min\\limits_{U,I}\\sum\\limits_{(u,i)} \\underbrace{(r_{ui} -  (I_i^TU_u + \\mu+ \\mu_i+\\mu_u))^2}_\\text{minimization} + \\underbrace{\\lambda(||U_u||^2+||I_u||^2 + \\mu  + \\mu_i+\\mu_u) }_\\text{regularization} $$\n",
    "\n",
    "\n",
    "### $$r_{ui} = \\mu + \\mu_i + \\mu_u + U_u.I_i $$\n",
    "\n",
    "### TODO:\n",
    "\n",
    "- (a) complete the model initialization\n",
    "- (b) complete the forward method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# <CORRECTION>\n",
    "class KorenMF(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,nb_users,nb_items,latent_size):\n",
    "        super(KorenMF, self).__init__()\n",
    "        \n",
    "        self.users = torch.nn.Embedding(nb_users, latent_size)\n",
    "        self.items = torch.nn.Embedding(nb_items, latent_size)\n",
    "        self.umean = torch.nn.Embedding(nb_users, 1)\n",
    "        self.imean = torch.nn.Embedding(nb_items, 1)\n",
    "        self.gmean = torch.nn.Parameter(torch.FloatTensor(1,).fill_(3))\n",
    "\n",
    "        torch.nn.init.normal_(self.users.weight,0,0.01)\n",
    "        torch.nn.init.normal_(self.items.weight,0,0.01)\n",
    "        torch.nn.init.normal_(self.umean.weight,2,1)\n",
    "        torch.nn.init.normal_(self.imean.weight,2,1)\n",
    "        \n",
    "        \n",
    "    def forward(self, user,item):\n",
    "        embed_u,embed_i = self.users(user).squeeze(1) , self.items(item).squeeze(1)\n",
    "        umean, imean = self.umean(user) , self.imean(item)\n",
    "        out = torch.sum(embed_u*embed_i,1) + umean.squeeze(-1) + imean.squeeze(-1) + self.gmean\n",
    "\n",
    "        return out , embed_u, embed_i, umean , imean , self.gmean\n",
    "# </CORRECTION>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (TODO) Here, train loop stays the same, you only have to change the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 16\n",
    "num_feat = 25\n",
    "lr = 0.01\n",
    "reg = 0.001\n",
    "\n",
    "# note: previous loss function should be robust to the new model thanks to advanced syntax :)\n",
    "\n",
    "model =  KorenMF(num_users,num_items,num_feat)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# same loop as before\n",
    "# <CORRECTION>\n",
    "for e in range(n_epochs):\n",
    "    mean_loss = [0,0,0] #train/val/test\n",
    "\n",
    "    for users_t,items_t,ratings_t in dataloader_train:\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        pred,*params = model(users_t,items_t)\n",
    "\n",
    "        loss,mse_loss = loss_func(pred,ratings_t,reg,*params)\n",
    "        loss.backward()\n",
    "        \n",
    "        mean_loss[0] += mse_loss\n",
    "        optimizer.step()\n",
    "    \n",
    "    \n",
    "\n",
    "    for users_t,items_t,ratings_t in dataloader_val:\n",
    "        model.eval()\n",
    "        pred,*params = model(users_t,items_t)\n",
    "        _,mse_loss = loss_func(pred,ratings_t,reg,*params)\n",
    "    \n",
    "        mean_loss[1] += mse_loss    \n",
    "        \n",
    "    for users_t,items_t,ratings_t in dataloader_test:\n",
    "        model.eval()\n",
    "        pred,*params = model(users_t,items_t)\n",
    "        _,mse_loss = loss_func(pred,ratings_t,reg,*params)\n",
    "    \n",
    "        mean_loss[2] += mse_loss    \n",
    "\n",
    "    print(\"-\"*25)\n",
    "    print(\"epoch\",e, \"mse (train/val/test)\", round((mean_loss[0]/len(prep_train)).item(),3),\"/\",  round((mean_loss[1]/len(prep_val)).item(),3),\"/\",  round((mean_loss[2]/len(prep_test)).item(),3))\n",
    "    # </CORRECTION>\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Optional part] How to complete this series of experiments\n",
    "\n",
    "### Visualization\n",
    "\n",
    "Use tsne to display embedding\n",
    "* could be done with sklearn [link](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)\n",
    "* often done with tensorboard in deep applications\n",
    "\n",
    "\n",
    "### Regularization\n",
    "\n",
    "Exploit side informations to regularize the profiles:\n",
    "* Users from the same age category are supposed to have closer representations, Movies from the same genre, etc...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load side informations\n",
    "uinfo = pd.read_csv(\"data/ml-100k/u.user\", sep=\"|\", names=[\"userId\",\"age\", \"genre\", \"prof\",\"zip\"])\n",
    "uinfo.head(5)\n",
    "\n",
    "# WARNING: we changed the definition of ids => make ids consistent\n",
    "uinfo[\"userId\"].map(user_map) # using the same dictionary\n",
    "genre_map = {g:num for num,g in enumerate(uinfo[\"genre\"].unique())}\n",
    "uinfo[\"genre\"].map(genre_map) \n",
    "prof_map = {p:num for num,p in enumerate(uinfo[\"prof\"].unique())}\n",
    "uinfo[\"prof\"].map(prof_map) \n",
    "# age cat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction du sujet à partir de la correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### <CORRECTION> ###\n",
    "import re\n",
    "# transformation de cet énoncé en version étudiante\n",
    "\n",
    "fname = \"3_1-reco-corr.ipynb\" # ce fichier\n",
    "fout  = fname.replace(\"-corr\",\"\")\n",
    "\n",
    "# print(\"Fichier de sortie: \", fout )\n",
    "\n",
    "f = open(fname, \"r\")\n",
    "txt = f.read()\n",
    " \n",
    "f.close()\n",
    "\n",
    "\n",
    "f2 = open(fout, \"w\")\n",
    "f2.write(re.sub(\"<CORRECTION>.*?(</CORRECTION>)\",\" TODO \",\\\n",
    "    txt, flags=re.DOTALL))\n",
    "f2.close()\n",
    "\n",
    "### </CORRECTION> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "902a52bcf4503a473db011f1937bdfe17613b08622219712e0110e48c4958c23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
