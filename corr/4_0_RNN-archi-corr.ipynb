{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN from scratch\n",
    "\n",
    "Notebook normalement peu utile si vous avez eu le cours... Sinon, c'est une manière d'aborder d'intercaler une marche avant le premier notebook sur les séries temporelles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import RNN, device,SampleMetroDataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "from torch.utils.data import DataLoader\n",
    "import logging\n",
    "import tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "from itertools import chain\n",
    "logging.basicConfig(level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Première idée: construire la cellule récurrente\n",
    "\n",
    "C'est ce qui est fait dans les notebooks des tuto officiels (remarquables) de pytorch:\n",
    "-   [NLP From Scratch: Classifying Names with a Character-Level\n",
    "    RNN](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)\n",
    "-   [NLP From Scratch: Generating Names with a Character-Level\n",
    "    RNN](https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On rappelle la structure classique en classification de signaux (many-to-one):\n",
    "\n",
    "![Image RNN](data/rnn_unfold2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du réseau\n",
    "\n",
    "1. Réseau correspondant au besoin ci-dessus\n",
    "2. Données arbitraires\n",
    "3. Instanciation du réseau + utilisation sur un exemple\n",
    "4. Passage dans une loss pour vérifier la cohérence des dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNN_V1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN_V1, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size, hidden_size) # W1\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size) # W2\n",
    "        self.h2o = nn.Linear(hidden_size, output_size) # W3\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden=None):\n",
    "        if hidden==None: hidden = self.initHidden() # init par défaut à 0\n",
    "        hidden = F.tanh(self.i2h(input) + self.h2h(hidden))\n",
    "        output = self.h2o(hidden)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "# 2. Data\n",
    "\n",
    "# data (x_1,...,x_T), batch, dimension des entrées\n",
    "#   dim1 : Seq. length T=23 \n",
    "#   dim2 : batch of size 10\n",
    "#   dim3 : Input x_t \\in R^n dimension 100\n",
    "seq_x = torch.rand(23, 10, 100) # simulation d'un batch\n",
    "y     = torch.randint(0,2,(10,))\n",
    "\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_hidden = 50\n",
    "d_data   = 100\n",
    "n_cl     = 2\n",
    "rnn = RNN_V1(d_data, n_hidden, n_cl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faire passer les données dans le réseau...\n",
    "\n",
    "Qu'est ce que j'ai le droit de donner à ce réseau??\n",
    "\n",
    "Avez-vous remarqué qu'il n'est pas récurrent??\n",
    "\n",
    "Sélectionner une propostion parmi les deux boites suivantes en réfléchissant aux conséquences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yhat:  torch.Size([23, 10, 2])\n",
      "hidden:  torch.Size([23, 10, 50])\n"
     ]
    }
   ],
   "source": [
    "# PROPOSITION 1:\n",
    "yhat, hidden = rnn(seq_x)\n",
    "\n",
    "print(\"yhat: \",yhat.size())\n",
    "print(\"hidden: \",hidden.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yhat:  torch.Size([10, 2])\n",
      "hidden:  torch.Size([10, 50])\n"
     ]
    }
   ],
   "source": [
    "# PROPOSITION 2:\n",
    "yhat, hidden = rnn(seq_x[0,:,:])\n",
    "\n",
    "print(\"yhat: \",yhat.size())\n",
    "print(\"hidden: \",hidden.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN et récurrence\n",
    "\n",
    "Evidemment, traiter une séquence entière sans prendre en compte les états cachés n'a aucun intérêt... Mais si on ne traite que le premier état observé, on n'a pas fait le travail complètement non plus...\n",
    "\n",
    "Complétons donc la solution précédente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yhat:  torch.Size([10, 2])\n",
      "hidden:  torch.Size([10, 50])\n"
     ]
    }
   ],
   "source": [
    "# Passage de l'ensemble de la séquence\n",
    "\n",
    "hidden = rnn.initHidden() # en dehors de la boucle\n",
    "for i in range(len(seq_x)):  # ca devrait marcher, le temps est stocké dans la première dim\n",
    "    yhat, hidden = rnn(seq_x[i,:,:], hidden) # iteration sur les états cachés calculés\n",
    "\n",
    "print(\"yhat: \",yhat.size())\n",
    "print(\"hidden: \",hidden.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN V2\n",
    "\n",
    "La solution actuelle est insatisfaisante: le `forward` est non standard, on ne peut pas intégrer cette architecture dans notre boucle classique de train...\n",
    "On a donc besoin d'une nouvelle version.\n",
    "\n",
    "Intégrer la boucle de récurrence à l'interieur du forward pour avoir:\n",
    "- une séquence en entrée\n",
    "- une séquence en sortie (toutes les couches cachées) + une sortie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_V2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN_V2, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size, hidden_size) # W1\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size) # W2\n",
    "        self.h2o = nn.Linear(hidden_size, output_size) # W3\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden=None):\n",
    "        if hidden==None: hidden = self.initHidden() # init par défaut à 0\n",
    "        allh = []\n",
    "\n",
    "        # calculer tous les états cachés\n",
    "        # les ajouter dans la liste\n",
    "        # Former un tensor avec tous les éléments de la liste (torch.cat)\n",
    "        ## <CORRECTION>\n",
    "        for i in range(len(input)):  # ca devrait marcher, le temps est stocké dans la première dim\n",
    "            hidden = F.tanh(self.i2h(input[i,:,:]) + self.h2h(hidden))\n",
    "            allh.append(hidden.unsqueeze(0))\n",
    "        allh = torch.cat(allh, dim=0)\n",
    "        ## </CORRECTION>\n",
    "      \n",
    "        output = self.h2o(hidden)\n",
    "        output = self.softmax(output)\n",
    "        return output, allh\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifier les dimensions attendues dans la boite ci-dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yhat:  torch.Size([10, 2])\n",
      "hidden:  torch.Size([23, 10, 50])\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN_V2(d_data, n_hidden, n_cl)\n",
    "\n",
    "yhat, hidden = rnn(seq_x)\n",
    "\n",
    "print(\"yhat: \",yhat.size())\n",
    "print(\"hidden: \",hidden.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN V3\n",
    "\n",
    "Utiliser un module prêt à l'emploi RNN, (on basculera ensuite vers des LSTM ou des GRU facilement)\n",
    "\n",
    "Après le rappel de cours, utiliser ce module à l'intérieur de votre proposition.\n",
    "\n",
    "[Lien vers la doc](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_h :  torch.Size([23, 10, 50])\n",
      "last_h:  torch.Size([1, 10, 50])\n"
     ]
    }
   ],
   "source": [
    "# Rappel de cours\n",
    "\n",
    "model = torch.nn.RNN(input_size=100, hidden_size=50, \n",
    "                     num_layers=1,\n",
    "                     nonlinearity='tanh', bias=True)\n",
    "\n",
    "# data (x_1,...,x_T), batch, dimension des entrées\n",
    "#   dim1 : Seq. length T=23 \n",
    "#   dim2 : batch of size 10\n",
    "#   dim3 : Input x_t \\in R^n dimension 100\n",
    "seq_x = torch.rand(23, 10, 100) # simulation d'un batch\n",
    "\n",
    "# For classification purpose, we use last_h\n",
    "seq_h, last_h = model(seq_x)\n",
    "\n",
    "print(\"seq_h : \",seq_h.size())\n",
    "print(\"last_h: \", last_h.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_V3(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN_V3, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rec = nn.RNN(input_size=input_size, hidden_size=hidden_size, \n",
    "                     num_layers=1,\n",
    "                     nonlinearity='tanh') # activation par défaut... On peut en tester d'autres\n",
    "        self.h2o = nn.Linear(hidden_size, output_size) # W3\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden=None):\n",
    "        \n",
    "        # calculer tous les états cachés d'un coup\n",
    "        # le RNN de torch accepte None comme valeur par défaut pour la couche cachée 0...\n",
    "        # 1 seule ligne est attendue dans la correction\n",
    "        ## <CORRECTION>\n",
    "        allh, hidden = self.rec(input, hidden)\n",
    "        ## </CORRECTION>\n",
    "      \n",
    "        output = self.h2o(hidden)\n",
    "        output = self.softmax(output)\n",
    "        return output, allh\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifier les dimensions attendues dans la boite ci-dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yhat:  torch.Size([1, 10, 2])\n",
      "hidden:  torch.Size([23, 10, 50])\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN_V3(d_data, n_hidden, n_cl)\n",
    "\n",
    "yhat, hidden = rnn(seq_x)\n",
    "\n",
    "print(\"yhat: \",yhat.size())\n",
    "print(\"hidden: \",hidden.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests en suspend\n",
    "\n",
    "- Tester le passage au GRU ou LSTM: attention, il y a une légère variation dans les arguments de sortie du `forward` de ces modèles.\n",
    "- Cette architecture n'est pas compatible avec les applications many-to-many ni avec la `CrossEntropyLoss`: envisager des solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction du sujet à partir de la correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### <CORRECTION> ###\n",
    "import re\n",
    "# transformation de cet énoncé en version étudiante\n",
    "\n",
    "fname = \"4_0_RNN-archi-corr.ipynb\" # ce fichier\n",
    "fout  = fname.replace(\"-corr\",\"\")\n",
    "\n",
    "# print(\"Fichier de sortie: \", fout )\n",
    "\n",
    "f = open(fname, \"r\")\n",
    "txt = f.read()\n",
    " \n",
    "f.close()\n",
    "\n",
    "f2 = open(fout, \"w\")\n",
    "f2.write(re.sub(\"<CORRECTION>.*?(</CORRECTION>)\",\" TODO \",\\\n",
    "    txt, flags=re.DOTALL))\n",
    "f2.close()\n",
    "\n",
    "### </CORRECTION> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyth-torch-numpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
