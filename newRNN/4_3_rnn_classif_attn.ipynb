{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification d'opinion avec un RNN \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import standard + \n",
    "# \n",
    "import numpy as np\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.autonotebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import re\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# device = \"cpu\"\n",
    "print(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from IPython.display import display, HTML\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Chemin vers TensorBoard\n",
    "TB_PATH = \"/tmp/logs/module-RNN\"\n",
    "\n",
    "# TENSORBOARD \n",
    "# usage externe de tensorboard: (1) lancer la commande dans une console; (2) copier-coller l'URL dans un navigateur\n",
    "display(HTML(\"<h2>Informations</h2><div>Pour visualiser les logs, tapez la commande : </div>\"))\n",
    "print(f\"tensorboard --logdir {Path(TB_PATH).absolute()}\")\n",
    "print(\"Une fois la commande lancer dans la console, copier-coller l'URL dans votre navigateur\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "A. Chargement des données\n",
    "------------------\n",
    "\n",
    "Tout le code est fourni. Le cadre est le même que pour la classification de noms: many-to-one. La tâche est de la classification d'opinion (sentiment en anglais)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GLOVE_PATH = Path(\"data/glove\")\n",
    "DATASET_PATH = Path(\"data/aclImdb\")\n",
    "IMDB_CLASSES  = ['neg','pos']\n",
    "\n",
    "class FolderText(Dataset):\n",
    "    \"\"\"Dataset basé sur des dossiers (un par classe) et fichiers\"\"\"\n",
    "\n",
    "    def __init__(self, classes, folder: Path, tokenizer, train_max_size = None, load=False):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.files = []\n",
    "        self.filelabels = []\n",
    "        self.labels = {}\n",
    "        for ix, key in enumerate(classes):\n",
    "            self.labels[key] = ix\n",
    "        \n",
    "        for label in classes:\n",
    "            c = 0\n",
    "            for file in (folder / label).glob(\"*.txt\"):\n",
    "                self.files.append(file.read_text() if load else file)\n",
    "                self.filelabels.append(self.labels[label])\n",
    "                c+=1\n",
    "                if train_max_size !=None and c>train_max_size: break\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filelabels)\n",
    "    \n",
    "    def __getitem__(self, ix):\n",
    "        s = self.files[ix]\n",
    "        return torch.tensor(self.tokenizer(s if isinstance(s, str) else s.read_text())), self.filelabels[ix]\n",
    "\n",
    "def get_imdb_data(embedding_size=50, train_max_size = None):\n",
    "    \"\"\"Renvoie l'ensemble des donnéees nécessaires pour l'apprentissage \n",
    "\n",
    "    - dictionnaire word vers ID\n",
    "    - embeddings (Glove)\n",
    "    - DataSet (FolderText)\n",
    "\n",
    "    \"\"\"\n",
    "    WORDS = re.compile(r\"\\S+\")\n",
    "    glove_fn = open(GLOVE_PATH / (\"glove.6B.%dd.txt\" % embedding_size))\n",
    "    words, embeddings = [], []\n",
    "    for line in glove_fn:\n",
    "        values = line.split()\n",
    "        words.append(values[0])\n",
    "        embeddings.append([float(x) for x in values[1:]])\n",
    "\n",
    "    OOVID = len(words)\n",
    "    words.append(\"__OOV__\")\n",
    "\n",
    "    word2id = {word: ix for ix, word in enumerate(words)}\n",
    "    embeddings = np.vstack((embeddings, np.zeros(embedding_size)))\n",
    "\n",
    "    def tokenizer(t):\n",
    "        return [word2id.get(x, OOVID) for x in re.findall(WORDS, t.lower())]\n",
    "\n",
    "    logging.info(\"Loading embeddings\")\n",
    "\n",
    "    logging.info(\"Get the IMDB dataset\")\n",
    "    \n",
    "\n",
    "    return word2id, embeddings, FolderText(IMDB_CLASSES, DATASET_PATH /\"train\", tokenizer, train_max_size, load=True), FolderText(IMDB_CLASSES, DATASET_PATH / \"test\", tokenizer, train_max_size, load=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id, embeddings, train_dataset, test_dataset = get_imdb_data(train_max_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prendre le temps de comprendre ce qui est chargé:\n",
    "\n",
    "- nature des informations\n",
    "- dimension des structures de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vérification des données chargées:\n",
    "\n",
    "word2id\n",
    "# embeddings\n",
    "# train_dataset[0]\n",
    "\n",
    "print(len(word2id), len(embeddings), len(embeddings[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passage au data-loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "    lengths = [len(seq) for seq in sequences]\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=False)\n",
    "    return padded_sequences, torch.tensor(lengths), torch.tensor(labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test\n",
    "\n",
    "batch =next(iter( train_loader))\n",
    "padded_sequences, lengths, labels = batch\n",
    "print(\"Padded sequences:\", padded_sequences)\n",
    "print(padded_sequences.size())\n",
    "# print(\"Lengths:\", lengths)\n",
    "print(\"Labels:\", labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Création du réseau\n",
    "\n",
    "Attention à la manière de gérer les embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RNNSent(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, embeddings):\n",
    "        super(RNNSent, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = len(embeddings[0])\n",
    "\n",
    "        self.emb = nn.Embedding(len(embeddings), len(embeddings[0]))\n",
    "\n",
    "        # INITIALISATION des embeddings\n",
    "        # 1. Récupération des valeurs présentes dans la structure (numpy) embeddings\n",
    "        # 2. Faut-il activer le gradient sur ce module? Dans la négative, comment le désactiver?\n",
    "        #  TODO \n",
    "\n",
    "        # CHOIX Du module récurrent\n",
    "        self.rec = nn.RNN(self.input_size,self.hidden_size, nonlinearity='tanh' )\n",
    "        # self.rec = nn.LSTM(self.input_size, self.hidden_size)\n",
    "\n",
    "        # ATTENTION à ajouter\n",
    "        # - réfléchir à la nature du module à utiliser\n",
    "        #  TODO \n",
    "\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, input, lengths=None):\n",
    "        # Principales étapes\n",
    "        # 1. translation of the input from int to emb\n",
    "        # 2. Passage dans le rec\n",
    "        # 3. Ajout de l'attention (non nécessaire dans un premier temps)\n",
    "        # 4. retour de la prediction sur la dernière couche\n",
    "\n",
    "         # print(\"input\", input.size())\n",
    "        maxlen = input.size(0)\n",
    "        batch_size = input.size(1)\n",
    "\n",
    "        # 1. translation of the input from int to emb\n",
    "        xemb = self.emb(input) \n",
    "        print(\"xemb\", xemb.size())\n",
    "\n",
    "        # 2. Passage dans le rec\n",
    "        hidden, last = self.rec(xemb)   # RNN\n",
    "        # hidden, (last,c) = self.rec(xemb) # LSTM => last[-1]\n",
    "        # print(\"last\", last.size())\n",
    "        print(\"hidden\", hidden.size())\n",
    "        \n",
    "        # recupération des dernières couches (réelles, sans padding)\n",
    "        if lengths != None:\n",
    "            last = torch.stack([hidden[ lengths[i] - 1, i, :] for i in range(batch_size)])\n",
    "    \n",
    "        # 3. Ajout de l'attention (non nécessaire dans un premier temps)\n",
    "        # dans la pratique, il s'agit d'une nouvelle manière de construire last\n",
    "        # WARNING: pour l'utilisation de batch, il faut savoir construire un masque\n",
    "        \n",
    "        # 3.1 passage dans l'attention\n",
    "        #  TODO \n",
    "\n",
    "        # 3.2 mask [à comprendre impérativemnent]\n",
    "        mask = torch.arange(maxlen).unsqueeze(1).expand(maxlen, batch_size) < lengths.unsqueeze(0)\n",
    "        print(mask)\n",
    "        masked_attn_scores = a.masked_fill(~mask, float('-inf'))\n",
    "\n",
    "        # 3.3 calcul de l'attention (utilisation du softmax) + application sur les couches cachées\n",
    "        #  TODO \n",
    "\n",
    "        output = self.h2o(last).squeeze(0)\n",
    "        #output = self.h2o(last) # LSTM\n",
    "        \n",
    "        return output, hidden\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# choose hidden size\n",
    "n_hidden = 128\n",
    "output_size = 2\n",
    "# build network\n",
    "rnn = RNNSent( n_hidden,  output_size, embeddings)\n",
    "rnn.name = \"RNNSent-\"+time.asctime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "# batch\n",
    "x, lengths, y = next(iter(train_loader))\n",
    "print(x.size(),y.size())\n",
    "\n",
    "# maxlen = x.size()[0]\n",
    "# mask = torch.arange(maxlen).unsqueeze(1).expand(maxlen, 100) < lengths.unsqueeze(0)\n",
    "# print(mask)\n",
    "\n",
    "yhat, hidden = rnn(x, lengths)\n",
    "print(yhat.size())\n",
    "l = loss(yhat,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Training\n",
    "\n",
    "1. put the data into a DataLoader\n",
    "2. choose a loss function \n",
    "3. run a standard training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# définition de la métrique d'évaluation\n",
    "def accuracy(yhat,y):\n",
    "    # y encode les indexes, s'assurer de la bonne taille de tenseur\n",
    "    assert len(y.shape)==1 or y.size(1)==1\n",
    "    return (torch.argmax(yhat,1).view(y.size(0),-1)== y.view(-1,1)).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def train(model,epochs,train_loader,test_loader):\n",
    "    writer = SummaryWriter(f\"{TB_PATH}/{model.name}\")\n",
    "    optim = torch.optim.Adam(model.parameters(),lr=1e-3)    # choix optimizer\n",
    "    model = model.to(device)\n",
    "    print(f\"running {model.name}\")\n",
    "    loss = nn.CrossEntropyLoss()                            # choix loss\n",
    "    # \n",
    "    # loss = nn.CrossEntropyLoss(weight=cl_weight.to(device))                            # choix loss\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        cumloss, cumacc, count = 0, 0, 0\n",
    "        model.train()\n",
    "        for x, lengths, y in train_loader:                            # boucle sur les batchs\n",
    "            optim.zero_grad()\n",
    "            x,y = x.to(device), y.to(device)                # y doit être un tensor (pas un int)\n",
    "            yhat, next_hidden = model(x, lengths)\n",
    "            l = loss(yhat,y)\n",
    "            l.backward()\n",
    "            optim.step()\n",
    "            cumloss += l*len(x)                             # attention, il peut y avoir un batch + petit (le dernier)\n",
    "            cumacc += accuracy(yhat,y)*len(x)\n",
    "            count += len(x)\n",
    "        writer.add_scalar('loss/train',cumloss/count,epoch)\n",
    "        writer.add_scalar('accuracy/train',cumacc/count,epoch)\n",
    "        if epoch % 2 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                cumloss, cumacc, count = 0, 0, 0\n",
    "                for x, lengths, y in test_loader:\n",
    "                    x,y = x.to(device), y.to(device)\n",
    "                    yhat, next_hidden = model(x, lengths)\n",
    "                    cumloss += loss(yhat,y)*len(x)\n",
    "                    cumacc += accuracy(yhat,y)*len(x)\n",
    "                    count += len(x)\n",
    "                writer.add_scalar(f'loss/test',cumloss/count,epoch)\n",
    "                writer.add_scalar('accuracy/test',cumacc/count,epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~10 minutes sur CPU\n",
    "n_epoch = 20\n",
    "train(rnn, n_epoch, train_loader, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Evaluating the Results\n",
    "\n",
    "From the qualitative point of view, then computing the confusion matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction du sujet à partir de la correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  TODO )\",\" TODO \",\\\n",
    "    txt, flags=re.DOTALL))\n",
    "f2.close()\n",
    "\n",
    "### </CORRECTION> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyth-torch-numpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
