{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architectures génératives à base de token\n",
    "\n",
    "Ce TP vise à utiliser directement des LSTM et à travailler sur des token pour traiter des données textuelles avec des approches de l'état de l'art.\n",
    "\n",
    "Une partie des codes nécessaires sont disponibles dans les fichiers annexes `textloader.py` et `generate.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import standard + \n",
    "# \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from textloader import *\n",
    "from generate import *\n",
    "\n",
    "\n",
    "import logging\n",
    "import time\n",
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# device = \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>Informations</h2><div>Pour visualiser les logs, tapez la commande : </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard --logdir /tmp/logs/module-RNN\n",
      "Une fois la commande lancer dans la console, copier-coller l'URL dans votre navigateur\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from IPython.display import display, HTML\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Chemin vers TensorBoard\n",
    "TB_PATH = \"/tmp/logs/module-RNN\"\n",
    "\n",
    "# TENSORBOARD \n",
    "# usage externe de tensorboard: (1) lancer la commande dans une console; (2) copier-coller l'URL dans un navigateur\n",
    "display(HTML(\"<h2>Informations</h2><div>Pour visualiser les logs, tapez la commande : </div>\"))\n",
    "print(f\"tensorboard --logdir {Path(TB_PATH).absolute()}\")\n",
    "print(\"Une fois la commande lancer dans la console, copier-coller l'URL dans votre navigateur\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Chargement des données\n",
    "\n",
    "Tout le code est fourni\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trump: Wow. Whoa. That is some group of people. Thousands. So nice, thank you very much. That's really nice. Thank you. It's great to be at Trump Tower. It's great to be in a wonderful city, New York. And it's an honor to have everybody here. This is beyond anybody's expectations. There's been no crowd like this. And, I can tell, some of the candidates, they went in. They didn't know the air-conditioner didn't work. They sweated like dogs. [laughter] They didn't know the room was too big, becaus\n"
     ]
    }
   ],
   "source": [
    "# paramétrisation\n",
    "\n",
    "DATA_PATH = \"./data/trump_full_speech.txt\"\n",
    "\n",
    "# OPERATION préliminaire: ouvrir le fichier pour comprendre la nature brute des données\n",
    "alltxts = Path(DATA_PATH).read_text()\n",
    "print(alltxts[:500]) # 100 premiers chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction du vocabulaire\n",
    "\n",
    "On introduit un *tokenizer* pour découper le texte en groupe de lettre. Ce découpage est non trivial (cf cours), on utilise un composant d'analyse venant des libs huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "# Initialiser un tokenizer WordPiece\n",
    "tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "\n",
    "# Définir les pré-traitements\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Créer un entraîneur avec un vocabulaire cible\n",
    "trainer = WordPieceTrainer(\n",
    "    vocab_size=10000,  # Limite du vocabulaire\n",
    "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train([DATA_PATH], trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# si on veut sauver/charger ces objets qui seront complètement liés aux modèles\n",
    "\n",
    "# # Sauvegarder le tokenizer entraîné\n",
    "# tokenizer.save(\"custom_tokenizer.json\")\n",
    "\n",
    "# # Charger et utiliser le tokenizer\n",
    "# from tokenizers import Tokenizer\n",
    "# tokenizer = Tokenizer.from_file(\"custom_tokenizer.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple important pour comprendre le principe de la tokenization et les inégalités entre langues, les potentiels problèmes avec du vocabulaire technique..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['This', 'is', 'an', 'example', '.']\n",
      "Token IDs: [504, 216, 279, 2015, 15]\n",
      "Tokens: ['Ex', '##emp', '##le', 'en', 'fr', '##anc', '##ais', ',', 'lo', '##in', 'de', 'Trump']\n",
      "Token IDs: [3064, 1573, 192, 357, 307, 1743, 8495, 13, 311, 167, 257, 457]\n"
     ]
    }
   ],
   "source": [
    "# Exemple de tokenisation\n",
    "output = tokenizer.encode(\"This is an example.\")\n",
    "print(\"Tokens:\", output.tokens)\n",
    "print(\"Token IDs:\", output.ids)\n",
    "\n",
    "# Autre exemple de tokenisation avec des mots plus rares\n",
    "output = tokenizer.encode(\"Exemple en francais, loin de Trump\")\n",
    "print(\"Tokens:\", output.tokens)\n",
    "print(\"Token IDs:\", output.ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Récupération des indices de tokens spéciaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAD 3\n",
      "EOS 15\n"
     ]
    }
   ],
   "source": [
    "PAD = tokenizer.encode(\"[PAD]\").ids[0]\n",
    "print(\"PAD\",PAD)\n",
    "\n",
    "EOS = tokenizer.encode(\".\").ids[0]\n",
    "print(\"EOS\", EOS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chaine de traitement\n",
    "\n",
    "1. Récupération des données avec un Dataset\n",
    "2. Construction du dataloader\n",
    "\n",
    "On se place dans une logique générative: on ne fait pas d'ensemble de test (c'est discutable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sys\n",
    "import re\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text: str, tokenizer, maxsent=None, maxlen=None):\n",
    "        \"\"\"  Dataset pour les tweets de Trump\n",
    "            * fname : nom du fichier\n",
    "            * tokenizer : tokenizer (text => [int])\n",
    "            * maxsent : nombre maximum de phrases.\n",
    "            * maxlen : longueur maximale des phrases.\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        maxlen = maxlen or sys.maxsize\n",
    "        self.phrases = [re.sub(' +',' ',p[:maxlen]).strip() +\".\" for p in text.split(\".\") if len(re.sub(' +',' ',p[:maxlen]).strip())>0]\n",
    "        if maxsent is not None:\n",
    "            self.phrases=self.phrases[:maxsent]\n",
    "        self.maxlen = max([len(p) for p in self.phrases])\n",
    "        self.phrasesnum = [torch.tensor(tokenizer.encode(p).ids) for p in self.phrases]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.phrases)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # return torch.tensor(tokenizer.encode(self.phrases[i])) # si on veut faire la transf à la volée\n",
    "        return self.phrasesnum[i]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = TextDataset(alltxts, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longueur: 17051\n",
      "doc 0:  tensor([ 457,   27, 3584,   15])\n",
      "doc 1:  tensor([9290,   15])\n",
      "doc 2:  tensor([ 508,  216,  627, 1450,  185,  270,   15])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"longueur:\", len(ds))\n",
    "print(\"doc 0: \", ds[0])\n",
    "print(\"doc 1: \", ds[1])\n",
    "print(\"doc 2: \", ds[2])\n",
    "\n",
    "# savez-vous retrouver les token EOS/.?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Trump : Wow .'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decoder une chaine\n",
    "tokenizer.decode(ds[0].tolist())\n",
    "\n",
    "# la fonction est jolie et enlève les éventuels ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "def collate_fn(batch):\n",
    "    #sequences, labels = zip(*batch)\n",
    "    lengths = [len(seq) for seq in batch]\n",
    "    padded_sequences = pad_sequence(batch, batch_first=False, padding_value=PAD)\n",
    "    return padded_sequences, torch.tensor(lengths)\n",
    "\n",
    "data = DataLoader(ds, collate_fn=collate_fn, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[  38, 1725,  296,  ...,  371,   38, 5626],\n",
      "        [  10,  534,  289,  ...,   10,   10,  383],\n",
      "        [  71,  313, 2375,  ...,  195,  417, 2497],\n",
      "        ...,\n",
      "        [   3,    3,    3,  ...,    3,    3,    3],\n",
      "        [   3,    3,    3,  ...,    3,    3,    3],\n",
      "        [   3,    3,    3,  ...,    3,    3,    3]]) torch.Size([47, 64])\n",
      "lengths: tensor([38, 17, 41, 10,  9, 28, 33, 25, 24, 18,  3, 11, 11,  9, 19, 25,  8,  8,\n",
      "        36, 12, 11, 17, 25, 16,  3,  9, 13,  7, 10, 24, 16, 31, 11,  6, 15,  9,\n",
      "         8, 25, 20, 24, 39, 16, 28,  9, 44, 15, 40, 29, 13, 26, 22, 20, 15, 37,\n",
      "        47, 22, 31,  6,  8,  5,  6,  8, 14,  4]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# recuperationn du premier batch et test sur la forme des sorties\n",
    "\n",
    "x,lengths = next(iter(data))\n",
    "print(\"x:\", x, x.size())\n",
    "print(\"lengths:\", lengths, lengths.size())\n",
    "#print(\"y:\", y, y.size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Creating the Network\n",
    "\n",
    "\n",
    "Implementing a RNN with a decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,  n_voc):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size  = input_size\n",
    "\n",
    "        self.emb = nn.Embedding( n_voc, input_size)\n",
    "        \n",
    "\n",
    "        # 1. Définition du module récurrent (self.rec)\n",
    "        # 2. Définition du décodeur (self.decoder)\n",
    "        # pas de piège, juste une réflexion sur les dimensions\n",
    "        ##  TODO \n",
    "        \n",
    "\n",
    "    def forward(self, input, hid=None):\n",
    "        # 1. translation of the input from int to one-hot\n",
    "        # 2. Passage dans le rec\n",
    "        # 3. retour de la prediction sur la dernière couche\n",
    "\n",
    "        ##  TODO \n",
    "        return output, last\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# choose hidden size\n",
    "input_size = 128\n",
    "hidden_size = 64\n",
    "# build network\n",
    "rnn = RNN( input_size, hidden_size,  tokenizer.get_vocab_size())\n",
    "rnn.name = \"baseRNN-\"+time.asctime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: torch.Size([62, 64, 10000])\n"
     ]
    }
   ],
   "source": [
    "# sanity check : est ce que les données passent dans le réseau? \n",
    "x,lengths = next(iter(data))\n",
    "output, hidden = rnn(x)\n",
    "\n",
    "print(\"output:\",output.size())\n",
    "# print(\"hidden:\", hidden.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'étape difficile maintenant:\n",
    "\n",
    "1. Calcul des yhat\n",
    "2. Mise en face des vérités terrains = le mot d'après\n",
    "\n",
    "... Mais il ne faut pas appliquer la loss sur le padding\n",
    "\n",
    "... Et on veut conserver un traitement sur les batchs $\\rightarrow$ introduire un masque sur la loss, il faut réfléchir en terme de graphe de calcul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskedCrossEntropy(output: torch.Tensor, target: torch.LongTensor, padcar: int):\n",
    "    \"\"\"\n",
    "    :param output: Tenseur length x batch x output_dim,  \n",
    "    :param target: Tenseur length x batch\n",
    "    :param padcar: index du caractere de padding\n",
    "    \"\"\"\n",
    "    mask = target != padcar\n",
    "    # print(mask)\n",
    "    f_mask = mask.flatten()\n",
    "    f_target = target.flatten()\n",
    "    f_output = output.view(-1, output.shape[2])\n",
    "    # print(f_mask.size(), f_target.size(), f_output.size())\n",
    "    return (f_mask * nn.CrossEntropyLoss()(f_output, f_target)).sum() / mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([132, 64])\n",
      "y: torch.Size([132, 64])\n",
      "torch.Size([132, 64, 10000])\n"
     ]
    }
   ],
   "source": [
    "# batch\n",
    "x,lengths = next(iter(data))\n",
    "y = x[1:,:] \n",
    "x = x[:-1,:]\n",
    "print(\"x:\", x.size())\n",
    "print(\"y:\", y.size())\n",
    "# modif 3 \n",
    "# yhat, hidden = rnn(x, lengths)\n",
    "yhat, hidden = rnn(x)\n",
    "print(yhat.size())\n",
    "l = maskedCrossEntropy(yhat,y, PAD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Training\n",
    "\n",
    "1. put the data into a DataLoader\n",
    "2. choose a loss function \n",
    "3. run a standard training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# définition de la métrique d'évaluation\n",
    "def accuracy(yhat,y):\n",
    "    # y encode les indexes, s'assurer de la bonne taille de tenseur\n",
    "    assert len(y.shape)==1 or y.size(1)==1\n",
    "    return (torch.argmax(yhat,1).view(y.size(0),-1)== y.view(-1,1)).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def train(model,epochs,train_loader):\n",
    "    writer = SummaryWriter(f\"{TB_PATH}/{model.name}\")\n",
    "    optim = torch.optim.Adam(model.parameters(),lr=1e-3)    # choix optimizer\n",
    "    model = model.to(device)\n",
    "    print(f\"running {model.name}\")\n",
    "    # loss = nn.CrossEntropyLoss(weight=cl_weight.to(device))      # choix loss (test en modification 4, independant de la pondération )\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        cumloss, cumacc, count = 0, 0, 0\n",
    "        model.train()\n",
    "        # for x, lengths, y in train_loader:   # boucle sur les batchs\n",
    "        for x, lengths in train_loader:              # boucle batch de 1\n",
    "            y = x[1:,:] \n",
    "            x = x[:-1,:]\n",
    "            optim.zero_grad()\n",
    "            x,y = x.to(device), y.to(device)               # y doit être un tensor (pas un int)\n",
    "\n",
    "            yhat, hidden = model(x) # batch = 1\n",
    "            l = maskedCrossEntropy(yhat,y, PAD)\n",
    "            l.backward()\n",
    "            optim.step()\n",
    "            cumloss += l*len(x)                             # attention, il peut y avoir un batch + petit (le dernier)\n",
    "            # cumacc += accuracy(yhat,y)*len(x)\n",
    "            count += len(x)\n",
    "        writer.add_scalar('loss/train',cumloss/count,epoch)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running baseRNN-Tue Jan  7 13:14:36 2025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      " 90%|█████████ | 36/40 [24:20<02:32, 38.16s/it]"
     ]
    }
   ],
   "source": [
    "# ~10 minutes sur CPU\n",
    "n_epoch = 40 # pour testet, ensuite, on peut prendre 100\n",
    "train(rnn, n_epoch, data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_model(model,fichier): # pas de sauvegarde de l'optimiseur ici\n",
    "      \"\"\" sauvegarde du modèle dans fichier \"\"\"\n",
    "      state = {'model_state': model.state_dict()}\n",
    "      torch.save(state,fichier) # pas besoin de passer par pickle\n",
    " \n",
    "def load_model(fichier,model):\n",
    "      \"\"\" Si le fichier existe, on charge le modèle  \"\"\"\n",
    "      if os.path.isfile(fichier):\n",
    "          state = torch.load(fichier)\n",
    "          model.load_state_dict(state['model_state'])\n",
    "      else:\n",
    "           print(\"Erreur de chargement du fichier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sauvegarde du modèle\n",
    "\n",
    "path = \"./\"\n",
    "fichier = path+f\"model/{rnn.name}\"\n",
    "save_model(rnn.to(\"cpu\"),fichier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chargement\n",
    "path = \"./\"\n",
    "fichier = path+f\"model/LSTM-Trump\"\n",
    "\n",
    "load_model(fichier,rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Construction de phrases et analyse des résultats\n",
    "\n",
    "Il est important de mettre en place un beam search pour faire fonctionner ces algorithmes génératifs\n",
    "\n",
    "1. On commence par la version naïve\n",
    "2. On passe aux approches à base de faisceaux (beam-search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"immigration\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    x = torch.tensor(tokenizer.encode(sent).ids).unsqueeze(1)\n",
    "    # print(x)\n",
    "    for i in range(30): # longeur arbitraire\n",
    "        # 1. Faire passer x dans le rnn\n",
    "        # 2. Récupérer l'argmax du y\n",
    "        # 3. Concatener l'indice  à x\n",
    "        # TODO \n",
    "\n",
    "    # 4. Décoder la chaine générée\n",
    "    print(tokenizer.decode(x.squeeze().tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mise en place d'un beam search\n",
    "\n",
    "1. Tester une fonction de génération stochastique\n",
    "2. Utiliser le beam search\n",
    "3. Ajouter une fonction de bonification des phrases longues (lien)[https://opennmt.net/OpenNMT/translation/beam_search/#length-normalization]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(rnn,tokenizer, eos, start=\"\", maxlen=200):\n",
    "    \"\"\"  Fonction de génération (l'embedding et le decodeur être des fonctions du rnn).\n",
    "         Initialise le réseau avec start (ou à 0 si start est vide) \n",
    "         et génère une séquence de longueur maximale 200 ou qui s'arrête quand eos est généré. \n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        x = torch.tensor(tokenizer.encode(start).ids).unsqueeze(1)\n",
    "        # procédure identique à la boite précédente... A l'exception de:\n",
    "        # c = torch.multinomial(torch.softmax(y, dim=1), 1)[0] => tirage multinomial\n",
    "        # TODO \n",
    "\n",
    "    return tokenizer.decode(x.squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate(rnn,tokenizer, EOS, start=\"Trump is\", maxlen=30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class LengthPower():\n",
    "    def __init__(self, base=5, alpha=0.7):\n",
    "        self.base = base\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def lengthpower(self, length):\n",
    "        # From https://opennmt.net/OpenNMT/translation/beam_search/#length-normalization\n",
    "        return math.exp(self.alpha * (math.log(self.base + length) - math.log(self.base + 1)))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_beam(rnn, tokenizer, eos, k, start=\"\", maxlen=200):\n",
    "    \"\"\" \n",
    "        Génere une séquence en beam-search : à chaque itération, \n",
    "        on explore pour chaque candidat les k symboles les plus probables; \n",
    "        puis seuls les k meilleurs candidats de l'ensemble des séquences générées sont conservés \n",
    "        (au sens de la vraisemblance) pour l'itération suivante.\n",
    "    \"\"\"\n",
    "    x = torch.tensor(tokenizer.encode(start).ids).unsqueeze(1)\n",
    "    # print(x.size())\n",
    "    with torch.no_grad():\n",
    "        y,h = rnn(x)\n",
    "        # print(h.size())\n",
    "        if x.size()[0]>1:\n",
    "            y = y[-1,:,:]\n",
    "\n",
    "        candidates = [ (h, x, y,  0, len(x), False) ]\n",
    "\n",
    "        for i in range(maxlen):\n",
    "            new_candidates = []\n",
    "            for h, x, y, logp, length, is_eos in candidates:\n",
    "                if is_eos:\n",
    "                    new_candidates.append((None, x, y, logp, length, None))\n",
    "                else:\n",
    "                    values, indices = torch.log_softmax(y, dim=1).topk(k)\n",
    "                    for logp_c, c in zip(values[0], indices[0]):\n",
    "                        # print(LengthPower().lengthpower(length+1))\n",
    "                        new_candidates.append((h, x, y, (logp + logp_c)/LengthPower().lengthpower(length+1), length+1, c))\n",
    "\n",
    "            candidates = []\n",
    "            # print([[h[0].size(), x.size(), y.size(), logp, length, c] for h, x, y, logp, length, c in new_candidates])\n",
    "            for h, x, y, logp, length, c in sorted(new_candidates, key=lambda x: -x[3])[:k]:\n",
    "                if c is None:\n",
    "                    candidates.append((None, x, y, logp, length, True))\n",
    "                else:\n",
    "                    c = c.unsqueeze(0).unsqueeze(0)\n",
    "                    # print(x.size(), c.size())\n",
    "                    x = torch.cat((x, c),0)\n",
    "                    y,h = rnn(c, h)\n",
    "                    # y = y[-1,:,:]\n",
    "                    # state = rnn()\n",
    "                    candidates.append((h, x, y, logp, length, c[0][0] == eos))\n",
    "\n",
    "\n",
    "    return tokenizer.decode(candidates[0][1].squeeze().tolist())\n",
    "\n",
    "print(generate_beam(rnn,tokenizer, EOS, 5, start=\"Immigration\", maxlen=30))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction du sujet à partir de la correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  TODO )\",\" TODO \",\\\n",
    "    txt, flags=re.DOTALL))\n",
    "f2.close()\n",
    "\n",
    "### </CORRECTION> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyth-torch-numpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
