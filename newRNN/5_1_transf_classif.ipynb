{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification d'opinion avec un Transformer \n",
    "\n",
    "Téléchargez les données sur le site d'origine:\n",
    "[https://ai.stanford.edu/~amaas/data/sentiment/]\n",
    "\n",
    "Penser à dezipper le fichier d'embeddings présent dans le répertoire data.\n",
    "\n",
    "Ensuite, le TP est opérationnel :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import standard + \n",
    "# \n",
    "import numpy as np\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.autonotebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import re\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# device = \"cpu\"\n",
    "print(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from IPython.display import display, HTML\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Chemin vers TensorBoard\n",
    "TB_PATH = \"/tmp/logs/module-Transf\"\n",
    "\n",
    "# TENSORBOARD \n",
    "# usage externe de tensorboard: (1) lancer la commande dans une console; (2) copier-coller l'URL dans un navigateur\n",
    "display(HTML(\"<h2>Informations</h2><div>Pour visualiser les logs, tapez la commande : </div>\"))\n",
    "print(f\"tensorboard --logdir {Path(TB_PATH).absolute()}\")\n",
    "print(\"Une fois la commande lancer dans la console, copier-coller l'URL dans votre navigateur\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "A. Chargement des données\n",
    "------------------\n",
    "\n",
    "Tout le code est fourni. Le cadre est le même que pour la classification de noms: many-to-one. La tâche est de la classification d'opinion (sentiment en anglais)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GLOVE_PATH = Path(\"data/glove\")\n",
    "DATASET_PATH_TRAIN = Path(\"data/aclImdb/train\")\n",
    "DATASET_PATH_TEST = Path(\"data/aclImdb/test\")\n",
    "\n",
    "NB_DOC_MAX = 12500 # par classe\n",
    "IMDB_CLASSES  = ['neg','pos']\n",
    "VOC_SIZE = 10000\n",
    "BATCH_SIZE = 32\n",
    "MAX_CHAR_SIZE = 1000\n",
    "\n",
    "\n",
    "labels = dict(zip(IMDB_CLASSES,[0,1]))\n",
    "\n",
    "def load_data(datapath, classes, max_size=None):\n",
    "    txts = []\n",
    "    files = []\n",
    "    filelabels = []\n",
    "    for label in classes:\n",
    "        c = 0\n",
    "        new = [os.path.join(datapath / label, f) for f in os.listdir(datapath / label) if f.endswith(\".txt\")]\n",
    "        files += new\n",
    "        # filelabels += [labels[label]] * len(new) \n",
    "        for file in (datapath / label).glob(\"*.txt\"):\n",
    "            t = file.read_text()\n",
    "            txts.append(t if len(t)<MAX_CHAR_SIZE else t[:MAX_CHAR_SIZE])\n",
    "            filelabels.append(labels[label])\n",
    "            c+=1\n",
    "            if max_size !=None and c>=max_size: break\n",
    "\n",
    "    return txts, files, filelabels\n",
    "    #     c+=1\n",
    "    #     if train_max_size !=None and c>train_max_size: break\n",
    "\n",
    "\n",
    "txts, files, filelabels = load_data(DATASET_PATH_TRAIN, IMDB_CLASSES, max_size = NB_DOC_MAX)\n",
    "txts_test, files_test, filelabels_test = load_data(DATASET_PATH_TEST, IMDB_CLASSES, max_size = NB_DOC_MAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(files[0])\n",
    "print(txts[0])\n",
    "print(filelabels[0])\n",
    "print(len(files),len(txts),len(filelabels))\n",
    "# labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction du vocabulaire\n",
    "\n",
    "On introduit un *tokenizer* pour découper le texte en groupe de lettre. Ce découpage est non trivial (cf cours), on utilise un composant d'analyse venant des libs huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "# Initialiser un tokenizer WordPiece\n",
    "tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "\n",
    "# Définir les pré-traitements\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Créer un entraîneur avec un vocabulaire cible\n",
    "trainer = WordPieceTrainer(\n",
    "    vocab_size=VOC_SIZE,  # Limite du vocabulaire\n",
    "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train(files, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de tokenisation\n",
    "output = tokenizer.encode(\"This is an example.\", add_special_tokens=True)\n",
    "print(\"Tokens:\", output.tokens)\n",
    "print(\"Token IDs:\", output.ids)\n",
    "\n",
    "# Autre exemple de tokenisation avec des mots plus rares\n",
    "output = tokenizer.encode(\"Exemple en francais, loin de la base IMDB\")\n",
    "print(\"Tokens:\", output.tokens)\n",
    "print(\"Token IDs:\", output.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Récupération des indices de tokens spéciaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = tokenizer.encode(\"[PAD]\").ids[0]\n",
    "print(\"PAD\",PAD)\n",
    "\n",
    "CLS = tokenizer.encode(\"[CLS]\").ids[0]\n",
    "print(\"CLS\",CLS)\n",
    "\n",
    "EOS = tokenizer.encode(\".\").ids[0]\n",
    "print(\"EOS\", EOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fabrication de tous les codes (+ astuce pour ajouter les CLS)\n",
    "\n",
    "allcodes = [torch.tensor(tokenizer.encode(\"[CLS] \" + p).ids) for p in txts]\n",
    "allcodes_test = [torch.tensor(tokenizer.encode(\"[CLS] \" + p).ids) for p in txts_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chaine de traitement\n",
    "\n",
    "1. Récupération des données avec un Dataset\n",
    "2. Construction du dataloader\n",
    "\n",
    "On se place dans une logique générative: on ne fait pas d'ensemble de test (c'est discutable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sys\n",
    "import re\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts: list, labels):\n",
    "        self.labels = labels\n",
    "        self.phrasesnum = texts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.phrasesnum[i], torch.tensor(self.labels[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = TextDataset(allcodes,filelabels)\n",
    "ds_test  = TextDataset(allcodes_test,filelabels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"longueur:\", len(ds_train))\n",
    "print(\"doc 0: \", ds_train[0])\n",
    "print(\"doc 1: \", ds_train[1])\n",
    "print(\"doc 2: \", ds_train[2])\n",
    "\n",
    "# savez-vous retrouver les token EOS/.?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "    lengths = [len(seq) for seq in sequences]\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=False)\n",
    "    return padded_sequences, torch.tensor(lengths), torch.tensor(labels)\n",
    "\n",
    "train_loader = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collate_fn)\n",
    "test_loader = DataLoader(ds_test, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "batch =next(iter( train_loader))\n",
    "padded_sequences, lengths, labels = batch\n",
    "print(\"Padded sequences:\", padded_sequences)\n",
    "print(padded_sequences.size())\n",
    "# print(\"Lengths:\", lengths)\n",
    "print(\"Labels:\", labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Création du réseau\n",
    "\n",
    "1. Comprendre les positional embeddings\n",
    "    - prendre le temps d'afficher les dimensions et de comprendre comment ils vont être utilisés\n",
    "    - activer ou ne pas activer le gradient???\n",
    "    \n",
    "2. Construire le réseau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def generate_sinusoidal_embeddings(seq_len, d_model):\n",
    "    position = torch.arange(seq_len).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "    pe = torch.zeros(seq_len, d_model)\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pe = generate_sinusoidal_embeddings(100, 128)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(pe.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransSent(nn.Module):\n",
    "    def __init__(self, emb_size, voc_size, num_layers, num_heads, hidden_size_mlp , output_size, maxlen=1000):\n",
    "        super(TransSent, self).__init__()\n",
    "\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "        self.emb = nn.Embedding(voc_size, emb_size)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=emb_size, \n",
    "            nhead=num_heads, \n",
    "            dim_feedforward=hidden_size_mlp,\n",
    "            activation='relu'\n",
    "        )\n",
    "\n",
    "        # Création d'un TransformerEncoder avec plusieurs couches\n",
    "        self.trans = nn.TransformerEncoder(\n",
    "            self.encoder_layer, \n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        # Attention, seuls les modules sont envoyés vers les devices\n",
    "        # pour envoyer automatiquement les tenseurs, il faut les \"enregistrer\"\n",
    "        self.register_buffer(\"posemb\", generate_sinusoidal_embeddings(maxlen, self.emb_size).unsqueeze(1))\n",
    "\n",
    "        # du CLS vers la classif\n",
    "        self.h2o = nn.Linear(emb_size, output_size)\n",
    "\n",
    "   \n",
    "    def forward(self, input, lengths=None):\n",
    "        # Principales étapes\n",
    "        # 1. translation of the input from int to emb\n",
    "        # 2. Passage dans le trans\n",
    "        # 3. Prediction sur le CLS\n",
    "\n",
    "        # print(\"input\", input.size())\n",
    "        maxlen = input.size(0)\n",
    "        batch_size = input.size(1)\n",
    "\n",
    "        # A analyser (et à utiliser plus tard)\n",
    "        padding_mask = (input[:, :] == PAD).T \n",
    "\n",
    "        # 1. translation of the input from int to emb + ajout des positional embeddings\n",
    "        #  TODO \n",
    "        \n",
    "        # 2. Passage dans le transformer... Avec le masque pour le padding\n",
    "        encoded_output = self.trans(xemb, src_key_padding_mask=padding_mask)\n",
    "        # print(\"encoded_output\", encoded_output.size())\n",
    "        \n",
    "        # 3. Appliquer la classification sur le CLS\n",
    "        #  TODO \n",
    "        \n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# choose hidden size\n",
    "emb_size = 128\n",
    "voc_size = VOC_SIZE\n",
    "num_layers = 4\n",
    "num_heads = 4\n",
    "hidden_size_mlp = 128\n",
    "output_size = 2\n",
    "# build network\n",
    "transf = TransSent( emb_size, voc_size, num_layers, num_heads, hidden_size_mlp , output_size)\n",
    "transf.name = \"TransSent-\"+time.asctime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "# batch\n",
    "x, lengths, y = next(iter(train_loader))\n",
    "print(x.size(),y.size())\n",
    "\n",
    "\n",
    "yhat = transf(x)\n",
    "print(yhat.size())\n",
    "l = loss(yhat,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Training\n",
    "\n",
    "1. put the data into a DataLoader\n",
    "2. choose a loss function \n",
    "3. run a standard training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# définition de la métrique d'évaluation\n",
    "def accuracy(yhat,y):\n",
    "    # y encode les indexes, s'assurer de la bonne taille de tenseur\n",
    "    assert len(y.shape)==1 or y.size(1)==1\n",
    "    return (torch.argmax(yhat,1).view(y.size(0),-1)== y.view(-1,1)).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def train(model,epochs,train_loader,test_loader):\n",
    "    writer = SummaryWriter(f\"{TB_PATH}/{model.name}\")\n",
    "    optim = torch.optim.Adam(model.parameters(),lr=5e-4)    # choix optimizer\n",
    "    model = model.to(device)\n",
    "    print(f\"running {model.name}\")\n",
    "    loss = nn.CrossEntropyLoss()                            # choix loss\n",
    "    # \n",
    "    # loss = nn.CrossEntropyLoss(weight=cl_weight.to(device))                            # choix loss\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        cumloss, cumacc, count = 0, 0, 0\n",
    "        model.train()\n",
    "        for x, lengths, y in train_loader:                            # boucle sur les batchs\n",
    "            optim.zero_grad()\n",
    "            x,y = x.to(device), y.to(device)                # y doit être un tensor (pas un int)\n",
    "            yhat = model(x)\n",
    "            l = loss(yhat,y)\n",
    "            l.backward()\n",
    "            optim.step()\n",
    "            cumloss += l*len(x)                             # attention, il peut y avoir un batch + petit (le dernier)\n",
    "            cumacc += accuracy(yhat,y)*len(x)\n",
    "            count += len(x)\n",
    "        writer.add_scalar('loss/train',cumloss/count,epoch)\n",
    "        writer.add_scalar('accuracy/train',cumacc/count,epoch)\n",
    "        if epoch % 2 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                cumloss, cumacc, count = 0, 0, 0\n",
    "                for x, lengths, y in test_loader:\n",
    "                    x,y = x.to(device), y.to(device)\n",
    "                    yhat = model(x)\n",
    "                    cumloss += loss(yhat,y)*len(x)\n",
    "                    cumacc += accuracy(yhat,y)*len(x)\n",
    "                    count += len(x)\n",
    "                writer.add_scalar(f'loss/test',cumloss/count,epoch)\n",
    "                writer.add_scalar('accuracy/test',cumacc/count,epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~10 minutes sur CPU\n",
    "n_epoch = 20\n",
    "train(transf, n_epoch, train_loader, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_model(model,fichier): # pas de sauvegarde de l'optimiseur ici\n",
    "      \"\"\" sauvegarde du modèle dans fichier \"\"\"\n",
    "      state = {'model_state': model.state_dict()}\n",
    "      torch.save(state,fichier) # pas besoin de passer par pickle\n",
    " \n",
    "def load_model(fichier,model):\n",
    "      \"\"\" Si le fichier existe, on charge le modèle  \"\"\"\n",
    "      if os.path.isfile(fichier):\n",
    "          state = torch.load(fichier)\n",
    "          model.load_state_dict(state['model_state'])\n",
    "      else:\n",
    "           print(\"Erreur de chargement du fichier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sauvegarde du modèle\n",
    "\n",
    "path = \"./\"\n",
    "fichier = path+f\"model/{transf.name}\"\n",
    "save_model(transf.to(\"cpu\"),fichier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creation de la coquille pour charger le modèle\n",
    "emb_size = 128\n",
    "voc_size = VOC_SIZE\n",
    "num_layers = 1\n",
    "num_heads = 1\n",
    "hidden_size_mlp = 128\n",
    "output_size = 2\n",
    "# build network\n",
    "transf = TransSent( emb_size, voc_size, num_layers, num_heads, hidden_size_mlp , output_size)\n",
    "\n",
    "# chargement\n",
    "path = \"./\"\n",
    "fichier = path+f\"model/TransSent-1H-1L\"\n",
    "\n",
    "load_model(fichier,transf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Amélioration de l'architecture et analyse des résultats\n",
    "\n",
    "1. Tester l'augmentation du nombre de têtes\n",
    "2. Analyse la forme des attentions, comment trouver les mots qui contrinuent le plus au diagnostic?\n",
    "3. Utiliser TensorBoard pour visualiser les représentations de mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# essais qualitatifs\n",
    "# tester le modèle sur des exemples de chaines de caractères \n",
    "# (oblige à faire la tokenisation + CLS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher le texte des messages sur lesquels on fait des erreurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher la matrice d'attention (ou seulement la ligne CLS) pour une entrée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utiliser tensorboard pour afficher en 2D les embeddings des mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction du sujet à partir de la correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  TODO )\",\" TODO \",\\\n",
    "    txt, flags=re.DOTALL))\n",
    "f2.close()\n",
    "\n",
    "### </CORRECTION> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyth-torch-numpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
