{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tranformer et modèles pré-entrainés\n",
    "\n",
    "Tutorial sur les outils huggingface\n",
    "\n",
    "Nous continuons à travailler sur la classification d'opinion pour ce nouveau TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, pipeline\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Introduction aux composants de base de la librairie HuggingFace\n",
    "\n",
    "HuggingFace propose deux composants clés :\n",
    "- AutoTokenizer : pour préparer les données textuelles en un format compatible avec les modèles\n",
    "- AutoModelForSequenceClassification : pour charger un modèle pré-entrainé pour les tâches de classification\n",
    "\n",
    "2. Récupération d'un modèle pré-entrainé de classification de sentiment\n",
    "\n",
    "Utilisation d'un modèle léger pré-entrainé sur des tâches de classification de sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"  # Un modèle léger pour la classification de sentiments\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de phrases à classer\n",
    "sentences = [\n",
    "    \"I love this product!\",\n",
    "    \"This is the worst experience I've ever had.\",\n",
    "    \"It's okay, not great but not bad either.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisation des phrases\n",
    "inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Prédictions avec le modèle\n",
    "outputs = model(**inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilisation d'une pipeline pour simplifier le processus\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Prédictions directement avec la pipeline\n",
    "results = classifier(sentences)\n",
    "for result in results:\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCICE\n",
    "\n",
    "Faire passer les données des films dans cette chaine de traitement et évaluer les performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuner ces modèles avec des données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  Trainer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import os\n",
    "\n",
    "# Charger les données locales depuis des répertoires \"pos\" et \"neg\"\n",
    "def load_data_from_dirs(base_dir):\n",
    "    texts, labels = [], []\n",
    "    for label, sentiment in enumerate([\"neg\", \"pos\"]):\n",
    "        dir_path = os.path.join(base_dir, sentiment)\n",
    "        for file_name in os.listdir(dir_path):\n",
    "            file_path = os.path.join(dir_path, file_name)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                texts.append(f.read().strip())\n",
    "                labels.append(label)\n",
    "    return Dataset.from_dict({\"text\": texts, \"label\": labels})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Charger les ensembles d'entraînement et de test\n",
    "train_dataset = load_data_from_dirs(\"data/aclimdb/train\")\n",
    "test_dataset = load_data_from_dirs(\"data/aclimdb/test\")\n",
    "\n",
    "# Créer un DatasetDict pour HuggingFace Trainer\n",
    "datasets = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenisation des données\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Définir les arguments d'entraînement\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",          # dossier de sortie\n",
    "    eval_strategy=\"epoch\",    # stratégie d'évaluation\n",
    "    learning_rate=2e-5,              # taux d'apprentissage\n",
    "    per_device_train_batch_size=16,  # taille des batches pour l'entraînement\n",
    "    per_device_eval_batch_size=16,   # taille des batches pour l'évaluation\n",
    "    num_train_epochs=1,              # nombre d'époques\n",
    "    weight_decay=0.01,               # décroissance de poids\n",
    "    logging_dir=\"./logs\",           # dossier de logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Initialisation du Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Entraînement\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Évaluation\n",
    "trainer.evaluate()\n",
    "\n",
    "# 4. Utilisation du modèle fine-tuné pour des prédictions\n",
    "# Exemple de phrases à classer\n",
    "sentences = [\n",
    "    \"I love this product!\",\n",
    "    \"This is the worst experience I've ever had.\",\n",
    "    \"It's okay, not great but not bad either.\"\n",
    "]\n",
    "\n",
    "# Utilisation d'une pipeline pour simplifier le processus\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Prédictions directement avec la pipeline\n",
    "results = classifier(sentences)\n",
    "for result in results:\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction du sujet à partir de la correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  TODO )\",\" TODO \",\\\n",
    "    txt, flags=re.DOTALL))\n",
    "f2.close()\n",
    "\n",
    "### </CORRECTION> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyth-torch-numpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
