{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification d'opinion avec un Transformer Ollama\n",
    "\n",
    "Téléchargez les données sur le site d'origine:\n",
    "[https://ai.stanford.edu/~amaas/data/sentiment/]\n",
    "\n",
    "Penser à dezipper le fichier d'embeddings présent dans le répertoire data.\n",
    "\n",
    "Ensuite, le TP est opérationnel :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import standard + \n",
    "# \n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import re\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# device = \"cpu\"\n",
    "print(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "A. Chargement des données\n",
    "------------------\n",
    "\n",
    "Tout le code est fourni. Le cadre est le même que pour la classification de noms: many-to-one. La tâche est de la classification d'opinion (sentiment en anglais)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATASET_PATH_TRAIN = Path(\"data/aclImdb/train\")\n",
    "DATASET_PATH_TEST = Path(\"data/aclImdb/test\")\n",
    "\n",
    "NB_DOC_MAX = 1000 # par classe\n",
    "IMDB_CLASSES  = ['neg','pos']\n",
    "VOC_SIZE = 10000\n",
    "BATCH_SIZE = 32\n",
    "MAX_CHAR_SIZE = 1000\n",
    "\n",
    "\n",
    "labels = dict(zip(IMDB_CLASSES,[0,1]))\n",
    "\n",
    "def load_data(datapath, classes, max_size=None):\n",
    "    txts = []\n",
    "    files = []\n",
    "    filelabels = []\n",
    "    for label in classes:\n",
    "        c = 0\n",
    "        new = [os.path.join(datapath / label, f) for f in os.listdir(datapath / label) if f.endswith(\".txt\")]\n",
    "        files += new\n",
    "        # filelabels += [labels[label]] * len(new) \n",
    "        for file in (datapath / label).glob(\"*.txt\"):\n",
    "            t = file.read_text()\n",
    "            txts.append(t if len(t)<MAX_CHAR_SIZE else t[:MAX_CHAR_SIZE])\n",
    "            filelabels.append(labels[label])\n",
    "            c+=1\n",
    "            if max_size !=None and c>=max_size: break\n",
    "\n",
    "    return txts, files, filelabels\n",
    "    #     c+=1\n",
    "    #     if train_max_size !=None and c>train_max_size: break\n",
    "\n",
    "\n",
    "txts, files, filelabels = load_data(DATASET_PATH_TRAIN, IMDB_CLASSES, max_size = NB_DOC_MAX)\n",
    "txts_test, files_test, filelabels_test = load_data(DATASET_PATH_TEST, IMDB_CLASSES, max_size = NB_DOC_MAX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Lien avec Ollama\n",
    "\n",
    "Charger l'API, un modèle et lancer un prompt\n",
    "\n",
    "Différents modes d'usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "response: ChatResponse = chat(model='phi3:latest', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the sky blue?',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])\n",
    "# or access fields directly from the response object\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation d'Ollama\n",
    "ollama.generate(model='phi3:latest', prompt='Why is the sky blue?')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tentative de contrainte du format de réponse\n",
    "\n",
    "Utiliser un prompt qui force le modèle à générer une réponse d'une certaine forme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_zero_shot(exemple):\n",
    "    prompt = f\"Give the sentiment of the following text: 'positive' or 'negative' :\\n\\n{exemple}\"\n",
    "    reponse = ollama.generate(model='phi3:latest',prompt=prompt)\n",
    "    return reponse\n",
    "\n",
    "print(txts[0])\n",
    "rep = classification_zero_shot(txts[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rep.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama.embed(model='phi3:latest', input='The sky is blue because of rayleigh scattering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyth-torch-numpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
