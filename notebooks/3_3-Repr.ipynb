{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "essential-dylan",
   "metadata": {},
   "source": [
    "# Apprentissage de représentation, Texte et recommendation\n",
    "\n",
    "## Partie B : Apprentissage de représentation, Fine-Tuning et Convolutions\n",
    "\n",
    "Nicolas Baskiotis (nicolas.baskiotis@sorbonne-univeriste.fr) Benjamin Piwowarski (benjamin.piwowarski@sorbonne-universite.fr) -- MLIA/ISIR, Sorbonne Université"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "decimal-accused",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-23e44fc3769d744b\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-23e44fc3769d744b\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6011;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "import torchdata # restart kernel after intall\n",
    "import torch\n",
    "assert torchtext.__version__ >= \"0.11.0\"\n",
    "from torchtext.datasets import IMDB # recuperation directe des données\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.utils.data import  DataLoader\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "TB_PATH = \"/tmp/logs/module3\"\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir  {TB_PATH}\n",
    "\n",
    "CACHEPATH = os.path.expanduser('~/.local/data')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# pour mac\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# torch.set_default_dtype(torch.float32) # pour GPU mac\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fitted-target",
   "metadata": {},
   "source": [
    "# Prise en main des outils textes pour le deep-learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "natural-reservation",
   "metadata": {},
   "source": [
    "L'utilisation la plus courante de l'apprentissage de représentation est de fournir une représentation utile pour la classification. Dans cette partie, nous utiliserons le corpus de critiques IMDB pour l'analyse de sentiment, un corpus qui contient des critiques de film et dont l'objectif est de prédire si une critique est  positive ou négative. Nous utiliserons pour la représentation d'une phrase le modèle moyen introduit ci-dessus : la moyenne des vecteurs de représentation des tokens qui la constitue. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6dd5a808",
   "metadata": {},
   "source": [
    "Explication du fonctionnement de la tokenization + padding sur un exemple jouet\n",
    "\n",
    "=> On fera ensuite la même chose sur les données IMBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fec3ce4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'cat', 'sat', 'on', 'the', 'mat'], ['the', 'dog', 'eat'], ['the', 'cat', 'and', 'the', 'dog']]\n",
      "Dictionnaire :  ['<pad>', '<oov>', 'the', 'cat', 'dog', 'and', 'eat', 'mat', 'on', 'sat']\n",
      "Taille du dictionnaire :  10\n",
      "Phrase index :  [tensor([2, 3, 9, 8, 2, 7]), tensor([2, 4, 6]), tensor([2, 3, 5, 2, 4])]\n",
      "reconstruction de la phrase :  ['the', 'cat', 'sat', 'on', 'the', 'mat']\n",
      "Phrase index pad:  tensor([[2, 3, 9, 8, 2, 7],\n",
      "        [2, 4, 6, 0, 0, 0],\n",
      "        [2, 3, 5, 2, 4, 0]])\n",
      "reconstruction de la phrase (pad) :  ['the', 'dog', 'eat', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\") # recuperation d'un tokenizer (= séparateur de mots)\n",
    "\n",
    "# test 1: tokenization d'une phrase:\n",
    "phrs = [\"the cat sat on the mat\", \"the dog eat\", \"the cat and the dog\"]\n",
    "phrs_tok = [tokenizer(t) for t in phrs]\n",
    "print(phrs_tok)\n",
    "\n",
    "# test 2: indexation des mots (ajout de mots speciaux)\n",
    "tok_ind = torchtext.vocab.build_vocab_from_iterator(phrs_tok, specials=['<pad>','<oov>'], min_freq=0)\n",
    "print(\"Dictionnaire : \", tok_ind.get_itos())\n",
    "\n",
    "print(\"Taille du dictionnaire : \", len(tok_ind))\n",
    "\n",
    "# test 3: indexation de la phrase \n",
    "phr_ind = [torch.tensor([tok_ind[t] for t in phr]) for phr in phrs_tok]\n",
    "print(\"Phrase index : \", phr_ind)\n",
    "\n",
    "# transformation inverse : phrase from index\n",
    "print(\"reconstruction de la phrase : \", [tok_ind.get_itos()[i] for i in phr_ind[0]])\n",
    "\n",
    "# test 4: padding: toutes les phrases doivent faire la même longueur\n",
    "phr_ind_pad = pad_sequence(phr_ind, padding_value=tok_ind['<pad>'])\n",
    "print(\"Phrase index pad: \", phr_ind_pad.T)\n",
    "\n",
    "print(\"reconstruction de la phrase (pad) : \", [tok_ind.get_itos()[i] for i in phr_ind_pad.T[1]])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "intellectual-funds",
   "metadata": {},
   "source": [
    "# DataLoader et Padding sur IMDB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "transparent-surprise",
   "metadata": {},
   "source": [
    "Dans un corpus de texte, les phrases n'ont généralement pas toutes la même longueur. Cela pose problème pour la constitution des mini-batchs sous forme de tenseurs, et en général pour tout opération tensorielle dans le réseau. Une solution simple consiste à faire du *padding*, c'est-à-dire rajouter à la fin des phrases un caractère spécial  *<pad>* pour les compléter  jusqu'à la longueur voulue. Ainsi les mini-batchs sont constitués de phrases de même longueur mais qui contiennent le caractère *<pad>*. On utilise généralement un vecteur nul pour sa représentation (et l'index 0). Comme ce caractère est à la fin du texte, il ne gène pas en général pas pour la phase forward d'inférence. Il suffit alors de \"masquer\" lors du calcul du coût - c'est-à-dire ne pas prendre en compte - les erreurs dûes à ce caractère. Un argument de **nn.CrossEntropy** permet justement d'ignorer les coûts dues à un index de classe spécifique.\n",
    "    \n",
    "Il faut donc faire **DataLoader** spécifique pour un tel corpus en charge de faire du padding pour constituer des batchs de même taille. L'argument ```collate_fn(batch)``` d'un **DataLoader** permet de spécifier une fonction en charge d'aggréger la liste ```batch``` des exemples individuels samplés dans le **Dataset** et de rendre le minibatch associé. La fonction ```collate_batch_imdb(batch)```  permet :\n",
    "* de convertir le texte de chaque élément du batch en liste d'indexes associés à chaque token du texte \n",
    "* de padder l'ensemble \n",
    "* de retourner le couple *(tenseur, étiquette)*. Vous pouvez utiliser la fonction ```torch.nn.utils.rnn.pad_sequence(l,batch_first,padding_value)``` qui permet de padder une liste de tenseurs (mettez ```batch_first=True``` pour que la fonction renvoie un tenseur dont la première dimension est le batch, la deuxième la longueur (et la troisième de dimension 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "confused-target",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données\n",
    "imdb_train_iter, imdb_test_iter = IMDB(root=CACHEPATH,split=('train','test'))\n",
    "imdb_train_list = list(imdb_train_iter)\n",
    "imdb_test_list = list(imdb_test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "common-attribute",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction du vocabulaire\n",
    "tokenizer = get_tokenizer(\"basic_english\") # recuperation d'un tokenizer (= séparateur de mots)\n",
    "def yield_imdb():\n",
    "    for _,l in imdb_train_list:\n",
    "        yield tokenizer(l) # yield = generation à la volée != stockage dans une liste\n",
    "\n",
    "# ajout de mots speciaux dans le vocabulaire => ils vont servir plus loin\n",
    "imdb_vocab = torchtext.vocab.build_vocab_from_iterator(yield_imdb(),specials=['<pad>','<oov>'],min_freq=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "horizontal-longitude",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du vocabulaire  13352\n",
      "['<pad>', '<oov>', 'the', '.', ',', 'and', 'a', 'of', 'to', \"'\", 'is', 'it', 'in', 'i', 'this', 'that', 's', 'was', 'as', 'for']\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# sur le vocabulaire IMDB\n",
    "print(\"Taille du vocabulaire \", len(imdb_vocab))\n",
    "print(imdb_vocab.get_itos()[:20]) # recup vocab (20 premiers mots)\n",
    "print(imdb_vocab['the'])    # recup index from vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "pleasant-politics",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mise en forme des textes, doit retourner le tenseur des indexes et le label\n",
    "def collate_batch_imdb(batch):\n",
    "    labels, texts = [],[]\n",
    "    for (label,text) in batch:\n",
    "        labels.append(1 if label==1 else 0)\n",
    "        texts.append(torch.tensor([imdb_vocab[t]   for t in tokenizer(text) if t in imdb_vocab]))\n",
    "    return pad_sequence(texts,batch_first=True, padding_value=imdb_vocab['<pad>']),torch.LongTensor(labels) \n",
    "    # attention à bien utiliser un mot spécial pour le padding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "infectious-address",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_train_dataloader = DataLoader(imdb_train_list,batch_size=32,shuffle=True,collate_fn=collate_batch_imdb)\n",
    "imdb_test_dataloader = DataLoader(imdb_test_list,batch_size=32,shuffle=True,collate_fn=collate_batch_imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "thrown-fundamentals",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2157,   12,   64,  ...,    0,    0,    0],\n",
       "         [  51,   26,  148,  ...,    0,    0,    0],\n",
       "         [1293,  674,    8,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [  14, 1306, 1698,  ...,    0,    0,    0],\n",
       "         [  65,  359,    8,  ...,    0,    0,    0],\n",
       "         [  51,   11,   45,  ...,    0,    0,    0]]),\n",
       " tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
       "         0, 0, 0, 1, 1, 0, 1, 1]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vérification du résultat\n",
    "next(iter(imdb_train_dataloader))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "organic-bryan",
   "metadata": {},
   "source": [
    "## <span class=\"alert-success\">  Exercice : Apprentissage End-to-End </span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "likely-limit",
   "metadata": {},
   "source": [
    "Le premier réseau que nous allons étudier un réseau avec une couche d'embeddings pour représenter les tokens et de plusieurs couches de linéaires avec des ReLU comme fonction d'activation.\n",
    "Il sera appris End-to-End dans un premier temps. \n",
    "\n",
    "\n",
    "Codez le réseau **ModeleMoyen** :  n'oubliez pas de prendre en compte le caractère **\\<pad\\>** dans la construction de la représentation de la phrase (en vrai cela n'a pas beaucoup d'importance dans ce cas précis, voyez-vous pourquoi ?).\n",
    "Vous pouvez utiliser 100 dimensions pour la représentation, et deux couches de 50 et de 20 neurones suivies de ReLU.\n",
    "\n",
    "<img src=\"./media/modaverage.png\">\n",
    "\n",
    "Entraînez le réseau, visualisez les embeddings obtenus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "japanese-percentage",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "def accuracy(yhat,y):\n",
    "    assert len(y.shape)==1 or y.size(1)==1\n",
    "    return (torch.argmax(yhat,1).view(y.size(0),-1)== y.view(-1,1)).float().mean()\n",
    "\n",
    "\n",
    "class ModeleMoyen(nn.Module):\n",
    "    # bien comprendre les arguments\n",
    "    def __init__(self,vocab_size,input_dim,output_dim,layers,padding_idx,pre_trained=None):\n",
    "        \"\"\"\n",
    "        vocab_size : taille vocabulaire\n",
    "        input_dim : taille des embeddings\n",
    "        output_dim : nb classes (ATTENTION, même dans le cas binaire, mettre 2 si loss = crossentropy)\n",
    "        layers : liste des dimensions des couches cachées [h_layer1, h_layer2, ...]\n",
    "        padding_idx : indice du mots de padding (à ne pas apprendre => emb_pad = [0,0,...0])\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # 0) save useful params\n",
    "        self.padding_idx = padding_idx # reference du mot special pad\n",
    "        # 1) il faut construire la table des représentation dans le réseau\n",
    "        if pre_trained is None:\n",
    "            self.embedding = nn.Embedding(vocab_size,input_dim,padding_idx=padding_idx) # pad n'est pas appris => 0\n",
    "        else:\n",
    "            self.embedding = nn.Embedding.from_pretrained(pre_trained, freeze=True, padding_idx=padding_idx)\n",
    "        # 2) construire les nlayers couches Linear + RELU\n",
    "        # 3) les mettre dans un Sequential\n",
    "        #       note: si listecouches = [couche1, couche2, ...] => self.fc = nn.Sequential(*listecouches)\n",
    "        ##  TODO \n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        # 1) recupération des embeddings des x\n",
    "        # 2) moyenne = somme / nombre de vrais mots (pas les pads)\n",
    "        # 3) forward du résultat\n",
    "        ##  TODO \n",
    "\n",
    "# tjs le même code (ou presque)\n",
    "def train(model,epochs,train_loader,test_loader):\n",
    "    writer = SummaryWriter(f\"{TB_PATH}/{model.name}\")\n",
    "    optim = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
    "    model = model.to(device)\n",
    "    print(f\"running {model.name}\")\n",
    "    loss = nn.CrossEntropyLoss() # ATTENTION: nout = 2 pour un problème binaire si crossentropie\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        cumloss, cumacc, count = 0, 0, 0\n",
    "        model.train()\n",
    "        for x,y in train_loader:\n",
    "            optim.zero_grad()\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            yhat = model(x)\n",
    "            l = loss(yhat,y)\n",
    "            l.backward()\n",
    "            optim.step()\n",
    "            cumloss += l*len(x)\n",
    "            cumacc += accuracy(yhat,y)*len(x) # calcul de la moyenne (avec des batchs inegaux)\n",
    "            count += len(x)\n",
    "        writer.add_scalar('loss/train',cumloss/count,epoch)\n",
    "        writer.add_scalar('accuracy/train',cumacc/count,epoch)\n",
    "        if epoch % 1 == 0: # on peut changer le pas pour gagner un peu de temps de calcul\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                cumloss, cumacc, count = 0, 0, 0\n",
    "                for x,y in test_loader:\n",
    "                    x,y = x.to(device), y.to(device)\n",
    "                    yhat = model(x)\n",
    "                    cumloss += loss(yhat,y)*len(x)\n",
    "                    cumacc += accuracy(yhat,y)*len(x)\n",
    "                    count += len(x)\n",
    "                writer.add_scalar(f'loss/test',cumloss/count,epoch)\n",
    "                writer.add_scalar('accuracy/test',cumacc/count,epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "conscious-newman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running modeleMoyen_Mon Jan  9 13:35:17 2023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:47<00:00, 22.70s/it]\n"
     ]
    }
   ],
   "source": [
    "## Création et entraînement du modèle moyen, visualisation des embeddings\n",
    "# 1) trouver les bons arguments pour construire votre réseau\n",
    "# 2) nommer le réseau (comme d'habitude)\n",
    "# 3) lancer train\n",
    "\n",
    "##  TODO \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f69136d",
   "metadata": {},
   "source": [
    "### Evaluation & Visualisation des embeddings de mots appris\n",
    "\n",
    "- les courbes d'apprentissage (et validation) doivent être très bonnes\n",
    "- les représentations de mots ont l'air d'avoir un peu de sens... Mais ce n'est pas très impressionnant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87994d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test d'un exemple custom\n",
    "phr = \"this movie is awful\" \n",
    "# phr = \"the movie is great\"\n",
    "input = torch.tensor([[imdb_vocab[t]   for t in tokenizer(phr) if t in imdb_vocab]])\n",
    "print(input)\n",
    "print(modeleMoyen(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbf4a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recherche de la première erreur\n",
    "\n",
    "for lab,txt in imdb_train_list:\n",
    "    input = torch.tensor([[imdb_vocab[t]   for t in tokenizer(txt) if t in imdb_vocab]])\n",
    "    pred = modeleMoyen(input).argmax().item()\n",
    "    # print(lab,pred)\n",
    "    if lab != pred:\n",
    "        print(lab,pred,txt)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-glass",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation des embeddings appris (50 premiers mots)\n",
    "writer = SummaryWriter(f\"{TB_PATH}/{modeleMoyen.name}\")\n",
    "writer.add_embedding(modeleMoyen.embedding.weight[:50],metadata=imdb_vocab.get_itos()[:50],global_step=3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "mighty-click",
   "metadata": {},
   "source": [
    "## Représentations pré-apprises et fine-tuning "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "vulnerable-collectible",
   "metadata": {},
   "source": [
    "Une option de **nn.Embeddings** permet de charger des représentations pré-entraînées. Dans ce cas, par défaut, la couche de représentation est gelée : elle n'est pas mise à jour durant l'apprentissage. On peut *fine-tuner* les représentations apprises, c'est-à-dire les adapter à notre tâche, en remettant à vrai le flag ```requires_grad``` du tenseur de représentation.\n",
    "\n",
    "Implémentez et testez les deux réseaux en utilisant ```fasttext.simple.300d```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "educated-positive",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Création du réseau modèle moyen à partir des représentation fasttext.simple.300d\n",
    "## Entraînement avec et sans fine-tuning.\n",
    "\n",
    "fasttext = torchtext.vocab.FastText('simple')\n",
    "fast_imdb_vectors = torch.stack([fasttext[m] for m in imdb_vocab.get_itos()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naval-scanner",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeleMoyenPretrained = ModeleMoyen(len(imdb_vocab),300,2,[50,20],imdb_vocab[\"<pad>\"],fast_imdb_vectors)\n",
    "modeleMoyenPretrained.name = \"pretrained_m\"+time.asctime()\n",
    "train(modeleMoyenPretrained,EPOCHS,imdb_train_dataloader,imdb_test_dataloader)\n",
    "\n",
    "# bien vérifier les performances dans tensorboard\n",
    "# Eventuellement comparer les embeddings dans tensorboard\n",
    "# reflechir sur le pourcentage des paramètres qui ont été mis à jour pendant l'apprentissage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-england",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeleMoyenPretrained_fine = ModeleMoyen(len(imdb_vocab),300,2,[50,20],imdb_vocab['<pad>'],fast_imdb_vectors)\n",
    "## Permet de réactiver l'apprentissage des représentations\n",
    "modeleMoyenPretrained_fine.embedding.weight.requires_grad = True # fine-tuning\n",
    "modeleMoyenPretrained_fine.name=\"pretrained_fine_m\"+time.asctime()\n",
    "train(modeleMoyenPretrained_fine,EPOCHS,imdb_train_dataloader,imdb_test_dataloader)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "occupied-mentor",
   "metadata": {},
   "source": [
    "# Convolution et texte"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "addressed-gazette",
   "metadata": {},
   "source": [
    "Le modèle précédent est relativement pauvre : le texte est représenté uniquement par le barycentre des représentations des tokens, ce qui élimine du coup toute notion de sequentialité. Une première manière de traiter la séquentialité est d'utiliser des couches convolutionnelles  et de pooling comme en image. Cette fois cependant, la convolution sera en une dimension et non pas deux.\n",
    "\n",
    "L'interaction entre l'opérateur de convolution et de max-pooling peut être vue de la manière suivante : \n",
    "\n",
    "    1. La convolution permet de détecter un ensemble de motifs (par exemple \"un bon film\", \"très bon film\", \"un film déplorable\") sur des séquences courtes qui dépend de la taille du noyau (dans cet exemple, 3 mots).\n",
    "    2. Le max-pooling permet de résumer l'information capturée par les filtres sur une sous-séquence de taille plus grande. \n",
    "    \n",
    "Ces opérations sont répétées comme en image plusieurs fois afin de détecter des motifs de taille et de complexité croissante. Par exemple, on peut détecter \"j'ai beaucoup\"/\"j'ai énormément\"/\"ai pas beaucoup\" et \"ce film\"/\"cet acteur\" (4 filtres) sur la première couche de convolution. La seconde couche de convolution va combiner les informations pour détecter un jugement sur un film ou un acteur.\n",
    "\n",
    "La dernière couche est généralement un maximum global (sur chaque filtre de sortie), suivie d'un classifieur linéaire.\n",
    "\n",
    "Codez une architecture convolutive pour les données IMDB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "arctic-glossary",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scope:\n",
    "    \"\"\"Permet de savoir quelle est la portée d'une couche\"\"\"\n",
    "    def __init__(self):\n",
    "        self.width = 1\n",
    "        self.stride = 1\n",
    "\n",
    "    def __call__(self, width, stride=1):\n",
    "        self.width = (width - 1) * self.stride + self.width\n",
    "        self.stride = self.stride * stride\n",
    "        return self\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"(w=%d,s=%d)\" % (self.width, self.stride)\n",
    "\n",
    "class ModelCNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, input_dim, output_dim, nfeat,padding_idx,pre_trained=None):\n",
    "        super().__init__()\n",
    "        self.nfeat = nfeat\n",
    "\n",
    "        if pre_trained is None:\n",
    "            self.embedding = nn.Embedding(vocab_size,input_dim,padding_idx=padding_idx)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding.from_pretrained(pre_trained, freeze=True, padding_idx=padding_idx)\n",
    "\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, 100, 3), # in, out, kernelsize\n",
    "            nn.MaxPool1d(3,1), # kernelsize, stride (=agregation 3 par 3) => longueur = longueur -2, nfiltres = cst\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(100, 100, 4),\n",
    "            nn.MaxPool1d(4,1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(100, self.nfeat, 6),\n",
    "            nn.MaxPool1d(4,1)\n",
    "        )\n",
    "\n",
    "        self.scope = Scope()\n",
    "        for m in self.convolutions:\n",
    "            if isinstance(m, torch.nn.modules.conv.Conv1d):\n",
    "                self.scope = self.scope(m.kernel_size[0], m.stride[0])\n",
    "            elif isinstance(m, torch.nn.modules.pooling.MaxPool1d):\n",
    "                self.scope = self.scope(m.kernel_size, m.stride)\n",
    "        print(\"Scope %s\", self.scope)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.nfeat, output_dim),\n",
    "        )\n",
    "\n",
    "    def convolution_fn(self, x: torch.Tensor):\n",
    "        x = self.embedding(x).transpose(1,2)\n",
    "        y = self.convolutions(x)\n",
    "        return y\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        y = self.convolution_fn(x)\n",
    "        z = self.fc(y.max(2)[0])\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "young-reset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scope %s (w=19,s=1)\n"
     ]
    }
   ],
   "source": [
    "modelCNN = ModelCNN(len(imdb_vocab),300,2,20,imdb_vocab[\"<pad>\"],fast_imdb_vectors)\n",
    "modelCNN.name = \"CNN\"+time.asctime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "clear-ranch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running CNNMon Jan  9 13:47:26 2023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [2:53:09<00:00, 1038.93s/it] \n"
     ]
    }
   ],
   "source": [
    "train(modelCNN,10,imdb_train_dataloader,imdb_test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-lebanon",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelCNN_fine = ModelCNN(len(imdb_vocab),300,2,20,imdb_vocab[\"<pad>\"],fast_imdb_vectors)\n",
    "modelCNN_fine.embedding.freeze = False\n",
    "modelCNN_fine.name = \"CNN-fine\"+time.asctime()\n",
    "train(modelCNN_fine,10,imdb_train_dataloader,imdb_test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1397613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_model(model,fichier): # pas de sauvegarde de l'optimiseur ici\n",
    "      \"\"\" sauvegarde du modèle dans fichier \"\"\"\n",
    "      state = {'model_state': model.state_dict()}\n",
    "      torch.save(state,fichier) # pas besoin de passer par pickle\n",
    " \n",
    "def load_model(fichier,model):\n",
    "      \"\"\" Si le fichier existe, on charge le modèle  \"\"\"\n",
    "      if os.path.isfile(fichier):\n",
    "          state = torch.load(fichier)\n",
    "          model.load_state_dict(state['model_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b014a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sauvegarde du réseau (économie de 30 minutes :)\n",
    "\n",
    "path = \"/Users/vguigue/Documents/Cours/Agro-IODAA/deep/notebooks/\"\n",
    "\n",
    "fichier = path+\"model/modelCNN\"\n",
    "save_model(modelCNN,fichier)\n",
    "\n",
    "fichier = path+\"model/modelCNN_fine\"\n",
    "save_model(modelCNN_fine,fichier)\n",
    "\n",
    "# vous pouvez utiliser les formules symmétriques pour le chargement\n",
    "\n",
    "# modelCNN = ModelCNN(len(imdb_vocab),300,2,20,imdb_vocab[\"<pad>\"],fast_imdb_vectors).to(device)\n",
    "# modelCNN.name =\"Conv-trained\"\n",
    "# modelCNN_fine = ModelCNN(len(imdb_vocab),300,2,20,imdb_vocab[\"<pad>\"],fast_imdb_vectors).to(device)\n",
    "# modelCNN_fine.name =\"Conv_fine-trained\"\n",
    "# load_model(\"model/modelCNN\", modelCNN)\n",
    "# load_model(\"model/modelCNN_fine\", modelCNN_fine)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "excellent-michigan",
   "metadata": {},
   "source": [
    "#### Afin d’étudier ce que fait le CNN, nous allons nous intéresser à la dernière couche avant le maximum global ; plus particulièrement, nous allons chercher les sous-séquences (dans le jeu de train) qui activent le plus chaque filtre de sortie.\n",
    "\n",
    "Pour cela, il faut tout d’abord déterminer à quelle position (dans le texte) correspond chaque sortie. Par exemple, si on considère une convolution avec une taille de noyau 3 et un stride de 1, alors la 1ème sortie correspondra au texte entre les positions 1 et 3, la 2ème à celui entre les positions 2 et 4, etc. Si on ajoute un max-pooling (noyau de taille 2, stride 2), alors la 1ère sortie correspond au texte entre les positions 1 et 4, la 2ème aux positions 3 et 6, etc.\n",
    "\n",
    "Il faut maintenant généraliser. Pour cela, nous allons considér que la ième opération (convolution/pooling) est définie seulement par la taille du noyau $w_i$ (*kernel width*) et le déplacement $s_i$ (*stride*). Nous nous intéressons aux deux valeurs qui caractérisent l’ensemble des transformations jusqu’a l’opération $i−1$ : \n",
    "    \n",
    "* La longueur des entrées correspondant à une sortie $W_{i−1}$.\n",
    "* Le déplacement $S_{i−1}$ dans les entrées correspondant à un déplacement unitaire dans les sorties.\n",
    "\n",
    "Donner la formule de récurrence qui permet, étant donné $W_i$ , $S_i$ de déterminer $W_{i+1}$\n",
    "et $S_{i+1}$ sachant $w_{i+1}$ et $s_{i+1}$.\n",
    "\n",
    "Soit $(y_1,\\ldots, y_L )$ la sortie du CNN. Une fois ce calcul fait, donnez la formule qui, étant\n",
    "donné la position j de la sortie $y_j$ , permet de déterminer les indices correspondant\n",
    "dans la séquence d’entrée.\n",
    "\n",
    "Finalement, parcourez les données du jeu de train, et trouvez les sous-séquences qui\n",
    "activent le plus chaque caractéristique de sortie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-cameroon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "# Un individu (texte + valeur)\n",
    "class Sample:\n",
    "    def __init__(self, value, text):\n",
    "        self.value = value\n",
    "        self.text = text\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Sample(%g)\" % self.value\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.value < other.value\n",
    "\n",
    "K = 20\n",
    "topk = [[] for i in range(modelCNN_fine.nfeat)]\n",
    "\n",
    "with torch.no_grad():\n",
    "        sampler = imdb_train_dataloader\n",
    "        for batch in sampler:\n",
    "            if batch[0].shape[1] < modelCNN_fine.scope.width:\n",
    "                continue\n",
    "            \n",
    "            # y a une taille batch x nfeat x lmax\n",
    "            y = modelCNN_fine.convolution_fn(batch[0].to(device))\n",
    "            bs, _nfeat, lmax = y.shape\n",
    "            assert _nfeat == modelCNN_fine.nfeat\n",
    "\n",
    "            # On cherche l'argmax sur toutes les activations\n",
    "            # On transpose d'abord (nfeat x batch x lmax)\n",
    "            yt = y.transpose(0,1).contiguous().view(modelCNN_fine.nfeat,-1)\n",
    "            _, indices = yt.topk(K, 1)\n",
    "            indices = indices.to('cpu')\n",
    "            \n",
    "            for i in range(modelCNN_fine.nfeat):\n",
    "                for k in range(indices.shape[1]):\n",
    "                    # On retrouve le batch et la position\n",
    "                    batch_ix = int(indices[i, k] // lmax)\n",
    "                    pos = int(indices[i, k] % lmax)\n",
    "\n",
    "                    # On retrouve le texte correspondant\n",
    "                    text = batch[0][batch_ix, pos * modelCNN_fine.scope.stride: pos*modelCNN_fine.scope.stride + modelCNN_fine.scope.width]\n",
    "                    sample = Sample(yt[i, indices[i, k]], text)\n",
    "\n",
    "                    if len(topk[i]) < K:\n",
    "                        heapq.heappush(topk[i], sample)\n",
    "                    elif topk[i][0].value < sample.value:\n",
    "                        heapq.heapreplace(topk[i], sample)\n",
    "\n",
    "for i in range(modelCNN_fine.nfeat):\n",
    "    print(\"==== Feat. %d ==== \" % i)\n",
    "    for sample in sorted(topk[i], reverse=True):\n",
    "        print(\"%.2f: %s\" % (float(sample.value), \" \".join(imdb_vocab.get_itos()[int(tid)] for tid in sample.text)))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b67b4318",
   "metadata": {},
   "source": [
    "# Construction du sujet à partir de la correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffe5b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###  TODO )\",\" TODO \",\\\n",
    "    txt, flags=re.DOTALL))\n",
    "f2.close()\n",
    "\n",
    "### </CORRECTION> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3257f100",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('torch-nightly')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "0539fa28f95d3c31ab10ef7831d62bf0e47751f442c28c9f64a267247464e7da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
